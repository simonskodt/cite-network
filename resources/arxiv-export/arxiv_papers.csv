title,author,summary,published
"PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary
  Views","Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Jiwen Lu","  We propose PixelGaussian, an efficient feed-forward framework for learning
generalizable 3D Gaussian reconstruction from arbitrary views. Most existing
methods rely on uniform pixel-wise Gaussian representations, which learn a
fixed number of 3D Gaussians for each view and cannot generalize well to more
input views. Differently, our PixelGaussian dynamically adapts both the
Gaussian distribution and quantity based on geometric complexity, leading to
more efficient representations and significant improvements in reconstruction
quality. Specifically, we introduce a Cascade Gaussian Adapter to adjust
Gaussian distribution according to local geometry complexity identified by a
keypoint scorer. CGA leverages deformable attention in context-aware
hypernetworks to guide Gaussian pruning and splitting, ensuring accurate
representation in complex regions while reducing redundancy. Furthermore, we
design a transformer-based Iterative Gaussian Refiner module that refines
Gaussian representations through direct image-Gaussian interactions. Our
PixelGaussian can effectively reduce Gaussian redundancy as input views
increase. We conduct extensive experiments on the large-scale ACID and
RealEstate10K datasets, where our method achieves state-of-the-art performance
with good generalization to various numbers of views. Code:
https://github.com/Barrybarry-Smith/PixelGaussian.
",2024-10-24T17:59:58Z
Framer: Interactive Frame Interpolation,"Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, Chunhua Shen","  We propose Framer for interactive frame interpolation, which targets
producing smoothly transitioning frames between two images as per user
creativity. Concretely, besides taking the start and end frames as inputs, our
approach supports customizing the transition process by tailoring the
trajectory of some selected keypoints. Such a design enjoys two clear benefits.
First, incorporating human interaction mitigates the issue arising from
numerous possibilities of transforming one image to another, and in turn
enables finer control of local motions. Second, as the most basic form of
interaction, keypoints help establish the correspondence across frames,
enhancing the model to handle challenging cases (e.g., objects on the start and
end frames are of different shapes and styles). It is noteworthy that our
system also offers an ""autopilot"" mode, where we introduce a module to estimate
the keypoints and refine the trajectory automatically, to simplify the usage in
practice. Extensive experimental results demonstrate the appealing performance
of Framer on various applications, such as image morphing, time-lapse video
generation, cartoon interpolation, etc. The code, the model, and the interface
will be released to facilitate further research.
",2024-10-24T17:59:51Z
"MotionCLR: Motion Generation and Training-free Editing via Understanding
  Attention Mechanisms","Ling-Hao Chen, Wenxun Dai, Xuan Ju, Shunlin Lu, Lei Zhang","  This research delves into the problem of interactive editing of human motion
generation. Previous motion diffusion models lack explicit modeling of the
word-level text-motion correspondence and good explainability, hence
restricting their fine-grained editing ability. To address this issue, we
propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR
modeling of attention mechanisms. Technically, MotionCLR models the in-modality
and cross-modality interactions with self-attention and cross-attention,
respectively. More specifically, the self-attention mechanism aims to measure
the sequential similarity between frames and impacts the order of motion
features. By contrast, the cross-attention mechanism works to find the
fine-grained word-sequence correspondence and activate the corresponding
timesteps in the motion sequence. Based on these key properties, we develop a
versatile set of simple yet effective motion editing methods via manipulating
attention maps, such as motion (de-)emphasizing, in-place motion replacement,
and example-based motion generation, etc. For further verification of the
explainability of the attention mechanism, we additionally explore the
potential of action-counting and grounded motion generation ability via
attention maps. Our experimental results show that our method enjoys good
generation and editing ability with good explainability.
",2024-10-24T17:59:45Z
CAMEL-Bench: A Comprehensive Arabic LMM Benchmark,"Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad S. Khan, Salman Khan, Rao M. Anwer","  Recent years have witnessed a significant interest in developing large
multimodal models (LMMs) capable of performing various visual reasoning and
understanding tasks. This has led to the introduction of multiple LMM
benchmarks to evaluate LMMs on different tasks. However, most existing LMM
evaluation benchmarks are predominantly English-centric. In this work, we
develop a comprehensive LMM evaluation benchmark for the Arabic language to
represent a large population of over 400 million speakers. The proposed
benchmark, named CAMEL-Bench, comprises eight diverse domains and 38
sub-domains including, multi-image understanding, complex visual perception,
handwritten document understanding, video understanding, medical imaging, plant
diseases, and remote sensing-based land use understanding to evaluate broad
scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions
that are filtered from a larger pool of samples, where the quality is manually
verified by native speakers to ensure reliable model assessment. We conduct
evaluations of both closed-source, including GPT-4 series, and open-source
LMMs. Our analysis reveals the need for substantial improvement, especially
among the best open-source models, with even the closed-source GPT-4o achieving
an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.
",2024-10-24T17:59:38Z
Unbounded: A Generative Infinite Game of Character Life Simulation,"Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz","  We introduce the concept of a generative infinite game, a video game that
transcends the traditional boundaries of finite, hard-coded systems by using
generative models. Inspired by James P. Carse's distinction between finite and
infinite games, we leverage recent advances in generative AI to create
Unbounded: a game of character life simulation that is fully encapsulated in
generative models. Specifically, Unbounded draws inspiration from sandbox life
simulations and allows you to interact with your autonomous virtual character
in a virtual world by feeding, playing with and guiding it - with open-ended
mechanics generated by an LLM, some of which can be emergent. In order to
develop Unbounded, we propose technical innovations in both the LLM and visual
generation domains. Specifically, we present: (1) a specialized, distilled
large language model (LLM) that dynamically generates game mechanics,
narratives, and character interactions in real-time, and (2) a new dynamic
regional image prompt Adapter (IP-Adapter) for vision models that ensures
consistent yet flexible visual generation of a character across multiple
environments. We evaluate our system through both qualitative and quantitative
analysis, showing significant improvements in character life simulation, user
instruction following, narrative coherence, and visual consistency for both
characters and the environments compared to traditional related approaches.
",2024-10-24T17:59:31Z
"3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D
  Generation","Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas","  Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
",2024-10-24T17:59:30Z
Tuning-free coreset Markov chain Monte Carlo,"Naitong Chen, Jonathan H. Huggins, Trevor Campbell","  A Bayesian coreset is a small, weighted subset of a data set that replaces
the full data during inference to reduce computational cost. The
state-of-the-art coreset construction algorithm, Coreset Markov chain Monte
Carlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the
coreset posterior to train the coreset weights via stochastic gradient
optimization. However, the quality of the constructed coreset, and thus the
quality of its posterior approximation, is sensitive to the stochastic
optimization learning rate. In this work, we propose a learning-rate-free
stochastic gradient optimization procedure, Hot-start Distance over Gradient
(Hot DoG), for training coreset weights in Coreset MCMC without user tuning
effort. Empirical results demonstrate that Hot DoG provides higher quality
posterior approximations than other learning-rate-free stochastic gradient
methods, and performs competitively to optimally-tuned ADAM.
",2024-10-24T17:59:23Z
"Deep Insights into Cognitive Decline: A Survey of Leveraging
  Non-Intrusive Modalities with Deep Learning Techniques","David Ortiz-Perez, Manuel Benavent-Lledo, Jose Garcia-Rodriguez, David Tomás, M. Flores Vizcaya-Moreno","  Cognitive decline is a natural part of aging, often resulting in reduced
cognitive abilities. In some cases, however, this decline is more pronounced,
typically due to disorders such as Alzheimer's disease. Early detection of
anomalous cognitive decline is crucial, as it can facilitate timely
professional intervention. While medical data can help in this detection, it
often involves invasive procedures. An alternative approach is to employ
non-intrusive techniques such as speech or handwriting analysis, which do not
necessarily affect daily activities. This survey reviews the most relevant
methodologies that use deep learning techniques to automate the cognitive
decline estimation task, including audio, text, and visual processing. We
discuss the key features and advantages of each modality and methodology,
including state-of-the-art approaches like Transformer architecture and
foundation models. In addition, we present works that integrate different
modalities to develop multimodal models. We also highlight the most significant
datasets and the quantitative results from studies using these resources. From
this review, several conclusions emerge. In most cases, the textual modality
achieves the best results and is the most relevant for detecting cognitive
decline. Moreover, combining various approaches from individual modalities into
a multimodal model consistently enhances performance across nearly all
scenarios.
",2024-10-24T17:59:21Z
ConceptDrift: Uncovering Biases through the Lens of Foundational Models,"Cristian Daniel Păduraru, Antonio Bărbălau, Radu Filipescu, Andrei Liviu Nicolicioiu, Elena Burceanu","  Datasets and pre-trained models come with intrinsic biases. Most methods rely
on spotting them by analysing misclassified samples, in a semi-automated
human-computer validation. In contrast, we propose ConceptDrift, a method which
analyzes the weights of a linear probe, learned on top a foundational model. We
capitalize on the weight update trajectory, which starts from the embedding of
the textual representation of the class, and proceeds to drift towards
embeddings that disclose hidden biases. Different from prior work, with this
approach we can pin-point unwanted correlations from a dataset, providing more
than just possible explanations for the wrong predictions. We empirically prove
the efficacy of our method, by significantly improving zero-shot performance
with biased-augmented prompting. Our method is not bounded to a single
modality, and we experiment in this work with both image (Waterbirds, CelebA,
Nico++) and text datasets (CivilComments).
",2024-10-24T17:59:16Z
Self-Improving Autonomous Underwater Manipulation,"Ruoshi Liu, Huy Ha, Mengxue Hou, Shuran Song, Carl Vondrick","  Underwater robotic manipulation faces significant challenges due to complex
fluid dynamics and unstructured environments, causing most manipulation systems
to rely heavily on human teleoperation. In this paper, we introduce AquaBot, a
fully autonomous manipulation system that combines behavior cloning from human
demonstrations with self-learning optimization to improve beyond human
teleoperation performance. With extensive real-world experiments, we
demonstrate AquaBot's versatility across diverse manipulation tasks, including
object grasping, trash sorting, and rescue retrieval. Our real-world
experiments show that AquaBot's self-optimized policy outperforms a human
operator by 41% in speed. AquaBot represents a promising step towards
autonomous and self-improving underwater manipulation systems. We open-source
both hardware and software implementation details.
",2024-10-24T17:59:14Z
"Ferret-UI 2: Mastering Universal User Interface Understanding Across
  Platforms","Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, Zhe Gan","  Building a generalist model for user interface (UI) understanding is
challenging due to various foundational issues, such as platform diversity,
resolution variation, and data limitation. In this paper, we introduce
Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI
understanding across a wide range of platforms, including iPhone, Android,
iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI
2 introduces three key innovations: support for multiple platform types,
high-resolution perception through adaptive scaling, and advanced task training
data generation powered by GPT-4o with set-of-mark visual prompting. These
advancements enable Ferret-UI 2 to perform complex, user-centered interactions,
making it highly versatile and adaptable for the expanding diversity of
platform ecosystems. Extensive empirical experiments on referring, grounding,
user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE
next-action prediction dataset, and GUI-World multi-platform benchmark
demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also
shows strong cross-platform transfer capabilities.
",2024-10-24T17:58:31Z
"Does Data Contamination Detection Work (Well) for LLMs? A Survey and
  Evaluation on Detection Assumptions","Yujuan Fu, Ozlem Uzuner, Meliha Yetisgen, Fei Xia","  Large language models (LLMs) have demonstrated great performance across
various benchmarks, showing potential as general-purpose task solvers. However,
as LLMs are typically trained on vast amounts of data, a significant concern in
their evaluation is data contamination, where overlap between training data and
evaluation datasets inflates performance assessments. While multiple approaches
have been developed to identify data contamination, these approaches rely on
specific assumptions that may not hold universally across different settings.
To bridge this gap, we systematically review 47 papers on data contamination
detection, categorize the underlying assumptions, and assess whether they have
been rigorously validated. We identify and analyze eight categories of
assumptions and test three of them as case studies. Our analysis reveals that
when classifying instances used for pretraining LLMs, detection approaches
based on these three assumptions perform close to random guessing, suggesting
that current LLMs learn data distributions rather than memorizing individual
instances. Overall, this work underscores the importance of approaches clearly
stating their underlying assumptions and testing their validity across various
scenarios.
",2024-10-24T17:58:22Z
On the Crucial Role of Initialization for Matrix Factorization,"Bingcong Li, Liang Zhang, Aryan Mokhtari, Niao He","  This work revisits the classical low-rank matrix factorization problem and
unveils the critical role of initialization in shaping convergence rates for
such nonconvex and nonsmooth optimization. We introduce Nystrom initialization,
which significantly improves the global convergence of Scaled Gradient Descent
(ScaledGD) in both symmetric and asymmetric matrix factorization tasks.
Specifically, we prove that ScaledGD with Nystrom initialization achieves
quadratic convergence in cases where only linear rates were previously known.
Furthermore, we extend this initialization to low-rank adapters (LoRA) commonly
used for finetuning foundation models. Our approach, NoRA, i.e., LoRA with
Nystrom initialization, demonstrates superior performance across various
downstream tasks and model scales, from 1B to 7B parameters, in large language
and diffusion models.
",2024-10-24T17:58:21Z
"Learning to Look: Seeking Information for Decision Making via Policy
  Factorization","Shivin Dass, Jiaheng Hu, Ben Abbatematteo, Peter Stone, Roberto Martín-Martín","  Many robot manipulation tasks require active or interactive exploration
behavior in order to be performed successfully. Such tasks are ubiquitous in
embodied domains, where agents must actively search for the information
necessary for each stage of a task, e.g., moving the head of the robot to find
information relevant to manipulation, or in multi-robot domains, where one
scout robot may search for the information that another robot needs to make
informed decisions. We identify these tasks with a new type of problem,
factorized Contextual Markov Decision Processes, and propose DISaM, a
dual-policy solution composed of an information-seeking policy that explores
the environment to find the relevant contextual information and an
information-receiving policy that exploits the context to achieve the
manipulation goal. This factorization allows us to train both policies
separately, using the information-receiving one to provide reward to train the
information-seeking policy. At test time, the dual agent balances exploration
and exploitation based on the uncertainty the manipulation policy has on what
the next best action is. We demonstrate the capabilities of our dual policy
solution in five manipulation tasks that require information-seeking behaviors,
both in simulation and in the real-world, where DISaM significantly outperforms
existing methods. More information at
https://robin-lab.cs.utexas.edu/learning2look/.
",2024-10-24T17:58:11Z
"OSCAR: Operating System Control via State-Aware Reasoning and
  Re-Planning","Xiaoqiang Wang, Bang Liu","  Large language models (LLMs) and large multimodal models (LMMs) have shown
great potential in automating complex tasks like web browsing and gaming.
However, their ability to generalize across diverse applications remains
limited, hindering broader utility. To address this challenge, we present
OSCAR: Operating System Control via state-Aware reasoning and Re-planning.
OSCAR is a generalist agent designed to autonomously navigate and interact with
various desktop and mobile applications through standardized controls, such as
mouse and keyboard inputs, while processing screen images to fulfill user
commands. OSCAR translates human instructions into executable Python code,
enabling precise control over graphical user interfaces (GUIs). To enhance
stability and adaptability, OSCAR operates as a state machine, equipped with
error-handling mechanisms and dynamic task re-planning, allowing it to
efficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's
effectiveness through extensive experiments on diverse benchmarks across
desktop and mobile platforms, where it transforms complex workflows into simple
natural language commands, significantly boosting user productivity. Our code
will be open-source upon publication.
",2024-10-24T17:58:08Z
"Where Am I and What Will I See: An Auto-Regressive Model for Spatial
  Localization and View Prediction","Junyi Chen, Di Huang, Weicai Ye, Wanli Ouyang, Tong He","  Spatial intelligence is the ability of a machine to perceive, reason, and act
in three dimensions within space and time. Recent advancements in large-scale
auto-regressive models have demonstrated remarkable capabilities across various
reasoning tasks. However, these models often struggle with fundamental aspects
of spatial reasoning, particularly in answering questions like ""Where am I?""
and ""What will I see?"". While some attempts have been done, existing approaches
typically treat them as separate tasks, failing to capture their interconnected
nature. In this paper, we present Generative Spatial Transformer (GST), a novel
auto-regressive framework that jointly addresses spatial localization and view
prediction. Our model simultaneously estimates the camera pose from a single
image and predicts the view from a new camera pose, effectively bridging the
gap between spatial awareness and visual prediction. The proposed innovative
camera tokenization method enables the model to learn the joint distribution of
2D projections and their corresponding spatial perspectives in an
auto-regressive manner. This unified training paradigm demonstrates that joint
optimization of pose estimation and novel view synthesis leads to improved
performance in both tasks, for the first time, highlighting the inherent
relationship between spatial awareness and visual prediction.
",2024-10-24T17:58:05Z
"Context is Key: A Benchmark for Forecasting with Essential Textual
  Information","Andrew Robert Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin","  Forecasting is a critical task in decision making across various domains.
While numerical data provides a foundation, it often lacks crucial context
necessary for accurate predictions. Human forecasters frequently rely on
additional information, such as background knowledge or constraints, which can
be efficiently communicated through natural language. However, the ability of
existing forecasting models to effectively integrate this textual information
remains an open question. To address this, we introduce ""Context is Key"" (CiK),
a time series forecasting benchmark that pairs numerical data with diverse
types of carefully crafted textual context, requiring models to integrate both
modalities. We evaluate a range of approaches, including statistical models,
time series foundation models, and LLM-based forecasters, and propose a simple
yet effective LLM prompting method that outperforms all other tested methods on
our benchmark. Our experiments highlight the importance of incorporating
contextual information, demonstrate surprising performance when using LLM-based
forecasting models, and also reveal some of their critical shortcomings. By
presenting this benchmark, we aim to advance multimodal forecasting, promoting
models that are both accurate and accessible to decision-makers with varied
technical expertise. The benchmark can be visualized at
https://servicenow.github.io/context-is-key-forecasting/v0/ .
",2024-10-24T17:56:08Z
"Stable Consistency Tuning: Understanding and Improving Consistency
  Models","Fu-Yun Wang, Zhengyang Geng, Hongsheng Li","  Diffusion models achieve superior generation quality but suffer from slow
generation speed due to the iterative nature of denoising. In contrast,
consistency models, a new generative family, achieve competitive performance
with significantly faster sampling. These models are trained either through
consistency distillation, which leverages pretrained diffusion models, or
consistency training/tuning directly from raw data. In this work, we propose a
novel framework for understanding consistency models by modeling the denoising
process of the diffusion model as a Markov Decision Process (MDP) and framing
consistency model training as the value estimation through Temporal
Difference~(TD) Learning. More importantly, this framework allows us to analyze
the limitations of current consistency training/tuning strategies. Built upon
Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),
which incorporates variance-reduced learning using the score identity. SCT
leads to significant performance improvements on benchmarks such as CIFAR-10
and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID
1.55, a new SoTA for consistency models.
",2024-10-24T17:55:52Z
"Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in
  Low-Resource Code","Jipeng Zhang, Jianshu Zhang, Yuanzhe Li, Renjie Pi, Rui Pan, Runtao Liu, Ziqiang Zheng, Tong Zhang","  Large Language Models (LLMs) demonstrate strong proficiency in generating
code for high-resource programming languages (HRPLs) like Python but struggle
significantly with low-resource programming languages (LRPLs) such as Racket or
D. This performance gap deepens the digital divide, preventing developers using
LRPLs from benefiting equally from LLM advancements and reinforcing disparities
in innovation within underrepresented programming communities. While generating
additional training data for LRPLs is promising, it faces two key challenges:
manual annotation is labor-intensive and costly, and LLM-generated LRPL code is
often of subpar quality. The underlying cause of this issue is the gap between
natural language to programming language gap (NL-PL Gap), which is especially
pronounced in LRPLs due to limited aligned data. In this work, we introduce a
novel approach called Bridge-Coder, which leverages LLMs' intrinsic
capabilities to enhance the performance on LRPLs. Our method consists of two
key stages. Bridge Generation, where we create high-quality dataset by
utilizing LLMs' general knowledge understanding, proficiency in HRPLs, and
in-context learning abilities. Then, we apply the Bridged Alignment, which
progressively improves the alignment between NL instructions and LRPLs.
Experimental results across multiple LRPLs show that Bridge-Coder significantly
enhances model performance, demonstrating the effectiveness and generalization
of our approach. Furthermore, we offer a detailed analysis of the key
components of our method, providing valuable insights for future work aimed at
addressing the challenges associated with LRPLs.
",2024-10-24T17:55:03Z
Large Spatial Model: End-to-end Unposed Images to Semantic 3D,"Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang","  Reconstructing and understanding 3D structures from a limited number of
images is a well-established problem in computer vision. Traditional methods
usually break this task into multiple subtasks, each requiring complex
transformations between different data representations. For instance, dense
reconstruction through Structure-from-Motion (SfM) involves converting images
into key points, optimizing camera parameters, and estimating structures.
Afterward, accurate sparse reconstructions are required for further dense
modeling, which is subsequently fed into task-specific neural networks. This
multi-step process results in considerable processing time and increased
engineering complexity.
  In this work, we present the Large Spatial Model (LSM), which processes
unposed RGB images directly into semantic radiance fields. LSM simultaneously
estimates geometry, appearance, and semantics in a single feed-forward
operation, and it can generate versatile label maps by interacting with
language at novel viewpoints. Leveraging a Transformer-based architecture, LSM
integrates global geometry through pixel-aligned point maps. To enhance spatial
attribute regression, we incorporate local context aggregation with multi-scale
fusion, improving the accuracy of fine local details. To tackle the scarcity of
labeled 3D semantic data and enable natural language-driven scene manipulation,
we incorporate a pre-trained 2D language-based segmentation model into a
3D-consistent semantic feature field. An efficient decoder then parameterizes a
set of semantic anisotropic Gaussians, facilitating supervised end-to-end
learning. Extensive experiments across various tasks show that LSM unifies
multiple 3D vision tasks directly from unposed images, achieving real-time
semantic 3D reconstruction for the first time.
",2024-10-24T17:54:42Z
"BioMistral-NLU: Towards More Generalizable Medical Language
  Understanding through Instruction Tuning","Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen","  Large language models (LLMs) such as ChatGPT are fine-tuned on large and
diverse instruction-following corpora, and can generalize to new tasks.
However, those instruction-tuned LLMs often perform poorly in specialized
medical natural language understanding (NLU) tasks that require domain
knowledge, granular text comprehension, and structured data extraction. To
bridge the gap, we: (1) propose a unified prompting format for 7 important NLU
tasks, % through span extraction and multi-choice question-answering (QA), (2)
curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing
open-source medical NLU corpora, and (3) develop BioMistral-NLU, a
generalizable medical NLU model, through fine-tuning BioMistral on
MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6
important NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical
Language Understanding Evaluation (BLUE) and Biomedical Language Understanding
and Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU
outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT
and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step
over diverse NLU tasks enhance LLMs' generalizability across diverse medical
NLU tasks. Our ablation experiments show that instruction-tuning on a wider
variety of tasks, even when the total number of training instances remains
constant, enhances downstream zero-shot generalization.
",2024-10-24T17:53:53Z
"Learning Structured Compressed Sensing with Automatic Resource
  Allocation","Han Wang, Eduardo Pérez, Iris A. M. Huijben, Hans van Gorp, Ruud van Sloun, Florian Römer","  Multidimensional data acquisition often requires extensive time and poses
significant challenges for hardware and software regarding data storage and
processing. Rather than designing a single compression matrix as in
conventional compressed sensing, structured compressed sensing yields
dimension-specific compression matrices, reducing the number of optimizable
parameters. Recent advances in machine learning (ML) have enabled task-based
supervised learning of subsampling matrices, albeit at the expense of complex
downstream models. Additionally, the sampling resource allocation across
dimensions is often determined in advance through heuristics. To address these
challenges, we introduce Structured COmpressed Sensing with Automatic Resource
Allocation (SCOSARA) with an information theory-based unsupervised learning
strategy. SCOSARA adaptively distributes samples across sampling dimensions
while maximizing Fisher information content. Using ultrasound localization as a
case study, we compare SCOSARA to state-of-the-art ML-based and greedy search
algorithms. Simulation results demonstrate that SCOSARA can produce
high-quality subsampling matrices that achieve lower Cram\'er-Rao Bound values
than the baselines. In addition, SCOSARA outperforms other ML-based algorithms
in terms of the number of trainable parameters, computational complexity, and
memory requirements while automatically choosing the number of samples per
axis.
",2024-10-24T17:53:33Z
The Learning Stabilizers with Noise problem,"Alexander Poremba, Yihui Quek, Peter Shor","  Random classical codes have good error correcting properties, and yet they
are notoriously hard to decode in practice. Despite many decades of extensive
study, the fastest known algorithms still run in exponential time. The Learning
Parity with Noise (LPN) problem, which can be seen as the task of decoding a
random linear code in the presence of noise, has thus emerged as a prominent
hardness assumption with numerous applications in both cryptography and
learning theory.
  Is there a natural quantum analog of the LPN problem? In this work, we
introduce the Learning Stabilizers with Noise (LSN) problem, the task of
decoding a random stabilizer code in the presence of local depolarizing noise.
We give both polynomial-time and exponential-time quantum algorithms for
solving LSN in various depolarizing noise regimes, ranging from extremely low
noise, to low constant noise rates, and even higher noise rates up to a
threshold. Next, we provide concrete evidence that LSN is hard. First, we show
that LSN includes LPN as a special case, which suggests that it is at least as
hard as its classical counterpart. Second, we prove a worst-case to
average-case reduction for variants of LSN. We then ask: what is the
computational complexity of solving LSN? Because the task features quantum
inputs, its complexity cannot be characterized by traditional complexity
classes. Instead, we show that the LSN problem lies in a recently introduced
(distributional and oracle) unitary synthesis class. Finally, we identify
several applications of our LSN assumption, ranging from the construction of
quantum bit commitment schemes to the computational limitations of learning
from quantum data.
",2024-10-24T17:53:02Z
Dynamic Vocabulary Pruning in Early-Exit LLMs,"Jort Vincenti, Karim Abdel Sadek, Joan Velja, Matteo Nulli, Metod Jazbec","  Increasing the size of large language models (LLMs) has been shown to lead to
better performance. However, this comes at the cost of slower and more
expensive inference. Early-exiting is a promising approach for improving the
efficiency of LLM inference by enabling next token prediction at intermediate
layers. Yet, the large vocabulary size in modern LLMs makes the confidence
estimation required for exit decisions computationally expensive, diminishing
the efficiency gains. To address this, we propose dynamically pruning the
vocabulary at test time for each token. Specifically, the vocabulary is pruned
at one of the initial layers, and the smaller vocabulary is then used
throughout the rest of the forward pass. Our experiments demonstrate that such
post-hoc dynamic vocabulary pruning improves the efficiency of confidence
estimation in early-exit LLMs while maintaining competitive performance.
",2024-10-24T17:52:31Z
Adjusted Overfitting Regression,Dylan Wilson,"  In this paper, I will introduce a new form of regression, that can adjust
overfitting and underfitting through, ""distance-based regression."" Overfitting
often results in finding false patterns causing inaccurate results, so by
having a new approach that minimizes overfitting, more accurate predictions can
be derived. Then I will proceed with a test of my regression form and show
additional ways to optimize the regression. Finally, I will apply my new
technique to a specific data set to demonstrate its practical value.
",2024-10-24T17:50:08Z
Path Guiding for Monte Carlo PDE Solvers,"Tianyu Huang, Jingwang Ling, Shuang Zhao, Feng Xu","  In recent years, Monte Carlo PDE solvers have garnered increasing attention
in computer graphics, demonstrating value across a wide range of applications.
Despite offering clear advantages over traditional methods-such as avoiding
discretization and enabling local evaluations-Monte Carlo PDE solvers face
challenges due to their stochastic nature, including high variance and slow
convergence rates. To mitigate the variance issue, we draw inspiration from
Monte Carlo path tracing and apply the path guiding technique to the Walk on
Stars estimator. Specifically, we examine the target sampling distribution at
each step of the Walk on Stars estimator, parameterize it, and introduce neural
implicit representations to model the spatially-varying guiding distribution.
This path guiding approach is implemented in a wavefront-style PDE solver, and
experimental results demonstrate that it effectively reduces variance in Monte
Carlo PDE solvers.
",2024-10-24T17:35:12Z
"A Random Matrix Theory Perspective on the Spectrum of Learned Features
  and Asymptotic Generalization Capabilities","Yatin Dandi, Luca Pesce, Hugo Cui, Florent Krzakala, Yue M. Lu, Bruno Loureiro","  A key property of neural networks is their capacity of adapting to data
during training. Yet, our current mathematical understanding of feature
learning and its relationship to generalization remain limited. In this work,
we provide a random matrix analysis of how fully-connected two-layer neural
networks adapt to the target function after a single, but aggressive, gradient
descent step. We rigorously establish the equivalence between the updated
features and an isotropic spiked random feature model, in the limit of large
batch size. For the latter model, we derive a deterministic equivalent
description of the feature empirical covariance matrix in terms of certain
low-dimensional operators. This allows us to sharply characterize the impact of
training in the asymptotic feature spectrum, and in particular, provides a
theoretical grounding for how the tails of the feature spectrum modify with
training. The deterministic equivalent further yields the exact asymptotic
generalization error, shedding light on the mechanisms behind its improvement
in the presence of feature learning. Our result goes beyond standard random
matrix ensembles, and therefore we believe it is of independent technical
interest. Different from previous work, our result holds in the challenging
maximal learning rate regime, is fully rigorous and allows for finitely
supported second layer initialization, which turns out to be crucial for
studying the functional expressivity of the learned features. This provides a
sharp description of the impact of feature learning in the generalization of
two-layer neural networks, beyond the random features and lazy training
regimes.
",2024-10-24T17:24:34Z
Matching Composition and Efficient Weight Reduction in Dynamic Matching,"Aaron Bernstein, Jiale Chen, Aditi Dudeja, Zachary Langley, Aaron Sidford, Ta-Wei Tu","  We consider the foundational problem of maintaining a
$(1-\varepsilon)$-approximate maximum weight matching (MWM) in an $n$-node
dynamic graph undergoing edge insertions and deletions. We provide a general
reduction that reduces the problem on graphs with a weight range of
$\mathrm{poly}(n)$ to $\mathrm{poly}(1/\varepsilon)$ at the cost of just an
additive $\mathrm{poly}(1/\varepsilon)$ in update time. This improves upon the
prior reduction of Gupta-Peng (FOCS 2013) which reduces the problem to a weight
range of $\varepsilon^{-O(1/\varepsilon)}$ with a multiplicative cost of
$O(\log n)$.
  When combined with a reduction of Bernstein-Dudeja-Langley (STOC 2021) this
yields a reduction from dynamic $(1-\varepsilon)$-approximate MWM in bipartite
graphs with a weight range of $\mathrm{poly}(n)$ to dynamic
$(1-\varepsilon)$-approximate maximum cardinality matching in bipartite graphs
at the cost of a multiplicative $\mathrm{poly}(1/\varepsilon)$ in update time,
thereby resolving an open problem in [GP'13; BDL'21]. Additionally, we show
that our approach is amenable to MWM problems in streaming, shared-memory
work-depth, and massively parallel computation models. We also apply our
techniques to obtain an efficient dynamic algorithm for rounding weighted
fractional matchings in general graphs. Underlying our framework is a new
structural result about MWM that we call the ""matching composition lemma"" and
new dynamic matching subroutines that may be of independent interest.
",2024-10-24T17:22:24Z
"Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent
  Role-Play","Sha Li, Revanth Gangi Reddy, Khanh Duy Nguyen, Qingyun Wang, May Fung, Chi Han, Jiawei Han, Kartik Natarajan, Clare R. Voss, Heng Ji","  Complex news events, such as natural disasters and socio-political conflicts,
require swift responses from the government and society. Relying on historical
events to project the future is insufficient as such events are sparse and do
not cover all possible conditions and nuanced situations. Simulation of these
complex events can help better prepare and reduce the negative impact. We
develop a controllable complex news event simulator guided by both the event
schema representing domain knowledge about the scenario and user-provided
assumptions representing case-specific conditions. As event dynamics depend on
the fine-grained social and cultural context, we further introduce a
geo-diverse commonsense and cultural norm-aware knowledge enhancement
component. To enhance the coherence of the simulation, apart from the global
timeline of events, we take an agent-based approach to simulate the individual
character states, plans, and actions. By incorporating the schema and cultural
norms, our generated simulations achieve much higher coherence and
appropriateness and are received favorably by participants from a humanitarian
assistance organization.
",2024-10-24T17:21:43Z
"ANAVI: Audio Noise Awareness using Visuals of Indoor environments for
  NAVIgation","Vidhi Jain, Rishi Veerapaneni, Yonatan Bisk","  We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for
quieter robot path planning. While humans are naturally aware of the noise they
make and its impact on those around them, robots currently lack this awareness.
A key challenge in achieving audio awareness for robots is estimating how loud
will the robot's actions be at a listener's location? Since sound depends upon
the geometry and material composition of rooms, we train the robot to passively
perceive loudness using visual observations of indoor environments. To this
end, we generate data on how loud an 'impulse' sounds at different listener
locations in simulated homes, and train our Acoustic Noise Predictor (ANP).
Next, we collect acoustic profiles corresponding to different actions for
navigation. Unifying ANP with action acoustics, we demonstrate experiments with
wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these
robots adhere to the noise constraints of the environment. See code and data at
https://anavi-corl24.github.io/
",2024-10-24T17:19:53Z
Sort-free Gaussian Splatting via Weighted Sum Rendering,"Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said","  Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant
advancement in 3D scene reconstruction, attracting considerable attention due
to its ability to recover high-fidelity details while maintaining low
complexity. Despite the promising results achieved by 3DGS, its rendering
performance is constrained by its dependence on costly non-commutative
alpha-blending operations. These operations mandate complex view dependent
sorting operations that introduce computational overhead, especially on the
resource-constrained platforms such as mobile phones. In this paper, we propose
Weighted Sum Rendering, which approximates alpha blending with weighted sums,
thereby removing the need for sorting. This simplifies implementation, delivers
superior performance, and eliminates the ""popping"" artifacts caused by sorting.
Experimental results show that optimizing a generalized Gaussian splatting
formulation to the new differentiable rendering yields competitive image
quality. The method was implemented and tested in a mobile device GPU,
achieving on average $1.23\times$ faster rendering.
",2024-10-24T17:18:01Z
AutoStep: Locally adaptive involutive MCMC,"Tiange Liu, Nikola Surjanovic, Miguel Biron-Lattes, Alexandre Bouchard-Côté, Trevor Campbell","  Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a
deterministic involutive proposal with a step size parameter. Selecting an
appropriate step size is often a challenging task in practice; and for complex
multiscale targets, there may not be one choice of step size that works well
globally. In this work, we address this problem with a novel class of
involutive MCMC methods -- AutoStep MCMC -- that selects an appropriate step
size at each iteration adapted to the local geometry of the target
distribution. We prove that AutoStep MCMC is $\pi$-invariant and has other
desirable properties under mild assumptions on the target distribution $\pi$
and involutive proposal. Empirical results examine the effect of various step
size selection design choices, and show that AutoStep MCMC is competitive with
state-of-the-art methods in terms of effective sample size per unit cost on a
range of challenging target distributions.
",2024-10-24T17:17:11Z
Learning $k$-body Hamiltonians via compressed sensing,"Muzhou Ma, Steven T. Flammia, John Preskill, Yu Tong","  We study the problem of learning a $k$-body Hamiltonian with $M$ unknown
Pauli terms that are not necessarily geometrically local. We propose a protocol
that learns the Hamiltonian to precision $\epsilon$ with total evolution time
${\mathcal{O}}(M^{1/2+1/p}/\epsilon)$ up to logarithmic factors, where the
error is quantified by the $\ell^p$-distance between Pauli coefficients. Our
learning protocol uses only single-qubit control operations and a GHZ state
initial state, is non-adaptive, is robust against SPAM errors, and performs
well even if $M$ and $k$ are not precisely known in advance or if the
Hamiltonian is not exactly $M$-sparse. Methods from the classical theory of
compressed sensing are used for efficiently identifying the $M$ terms in the
Hamiltonian from among all possible $k$-body Pauli operators. We also provide a
lower bound on the total evolution time needed in this learning task, and we
discuss the operational interpretations of the $\ell^1$ and $\ell^2$ error
metrics. In contrast to previous works, our learning protocol requires neither
geometric locality nor any other relaxed locality conditions.
",2024-10-24T17:16:19Z
"SafeBench: A Safety Evaluation Framework for Multimodal Large Language
  Models","Zonghao Ying, Aishan Liu, Siyuan Liang, Lei Huang, Jinyang Guo, Wenbo Zhou, Xianglong Liu, Dacheng Tao","  Multimodal Large Language Models (MLLMs) are showing strong safety concerns
(e.g., generating harmful outputs for users), which motivates the development
of safety evaluation benchmarks. However, we observe that existing safety
benchmarks for MLLMs show limitations in query quality and evaluation
reliability limiting the detection of model safety implications as MLLMs
continue to evolve. In this paper, we propose \toolns, a comprehensive
framework designed for conducting safety evaluations of MLLMs. Our framework
consists of a comprehensive harmful query dataset and an automated evaluation
protocol that aims to address the above limitations, respectively. We first
design an automatic safety dataset generation pipeline, where we employ a set
of LLM judges to recognize and categorize the risk scenarios that are most
harmful and diverse for MLLMs; based on the taxonomy, we further ask these
judges to generate high-quality harmful queries accordingly resulting in 23
risk scenarios with 2,300 multi-modal harmful query pairs. During safety
evaluation, we draw inspiration from the jury system in judicial proceedings
and pioneer the jury deliberation evaluation protocol that adopts collaborative
LLMs to evaluate whether target models exhibit specific harmful behaviors,
providing a reliable and unbiased assessment of content security risks. In
addition, our benchmark can also be extended to the audio modality showing high
scalability and potential. Based on our framework, we conducted large-scale
experiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,
GPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs
and instantiated several insights on MLLM safety performance such as image
quality and parameter size.
",2024-10-24T17:14:40Z
"LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor
  Search","Elias Jääsaari, Ville Hyvönen, Teemu Roos","  Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets.
",2024-10-24T17:13:39Z
"Swarm manipulation: An efficient and accurate technique for multi-object
  manipulation in virtual reality","Xiang Li, Jin-Du Wang, John J. Dudley, Per Ola Kristensson","  The theory of swarm control shows promise for controlling multiple objects,
however, scalability is hindered by cost constraints, such as hardware and
infrastructure. Virtual Reality (VR) can overcome these limitations, but
research on swarm interaction in VR is limited. This paper introduces a novel
Swarm Manipulation interaction technique and compares it with two baseline
techniques: Virtual Hand and Controller (ray-casting). We evaluated these
techniques in a user study ($N$ = 12) in three tasks (selection, rotation, and
resizing) across five conditions. Our results indicate that Swarm Manipulation
yielded superior performance, with significantly faster speeds in most
conditions across the three tasks. It notably reduced resizing size deviations
but introduced a trade-off between speed and accuracy in the rotation task.
Additionally, we conducted a follow-up user study ($N$ = 6) using Swarm
Manipulation in two complex VR scenarios and obtained insights through
semi-structured interviews, shedding light on optimized swarm control
mechanisms and perceptual changes induced by this interaction paradigm. These
results demonstrate the potential of the Swarm Manipulation technique to
enhance the usability and user experience in VR compared to conventional
manipulation techniques. In future studies, we aim to understand and improve
swarm interaction via internal swarm particle cooperation.
",2024-10-24T17:12:51Z
SegLLM: Multi-round Reasoning Segmentation,"XuDong Wang, Shaolun Zhang, Shufan Li, Konstantinos Kallidromitis, Kehan Li, Yusuke Kato, Kazuki Kozuka, Trevor Darrell","  We present SegLLM, a novel multi-round interactive reasoning segmentation
model that enhances LLM-based segmentation by exploiting conversational memory
of both visual and textual outputs. By leveraging a mask-aware multimodal LLM,
SegLLM re-integrates previous segmentation results into its input stream,
enabling it to reason about complex user intentions and segment objects in
relation to previously identified entities, including positional,
interactional, and hierarchical relationships, across multiple interactions.
This capability allows SegLLM to respond to visual and text queries in a
chat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM
outperforms existing methods in multi-round interactive reasoning segmentation
by over 20%. Additionally, we observed that training on multi-round reasoning
segmentation data enhances performance on standard single-round referring
segmentation and localization tasks, resulting in a 5.5% increase in cIoU for
referring expression segmentation and a 4.5% improvement in Acc@0.5 for
referring expression localization.
",2024-10-24T17:11:52Z
"How to Design a Quantum Streaming Algorithm Without Knowing Anything
  About Quantum Computing","John Kallaugher, Ojas Parekh, Nadezhda Voronova","  A series of work [GKK+08, Kal22, KPV24] has shown that asymptotic advantages
in space complexity are possible for quantum algorithms over their classical
counterparts in the streaming model. We give a simple quantum sketch that
encompasses all these results, allowing them to be derived from entirely
classical algorithms using our quantum sketch as a black box. The quantum
sketch and its proof of correctness are designed to be accessible to a reader
with no background in quantum computation, relying on only a small number of
self-contained quantum postulates.
",2024-10-24T17:11:37Z
"From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical
  Integrity on Faulty Mathematical Problems","A M Muntasir Rahman, Junyi Ye, Wei Yao, Wenpeng Yin, Guiling Wang","  Consider the math problem: ""Lily received 3 cookies from her best friend
yesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.
How many cookies does Lily have now?"" Many large language models (LLMs) in
previous research approach this problem by calculating the answer ""1"" using the
equation ""3 - 5 + 3."" However, from a human perspective, we recognize the
inherent flaw in this problem: Lily cannot eat 5 cookies if she initially only
had 3. This discrepancy prompts a key question: Are current LLMs merely Blind
Solver that apply mathematical operations without deeper reasoning, or can they
function as Logical Thinker capable of identifying logical inconsistencies?
  To explore this question, we propose a benchmark dataset, FaultyMath, which
includes faulty math problems of rich diversity: i) multiple mathematical
categories, e.g., algebra, geometry, number theory, etc., ii) varying levels of
difficulty, and iii) different origins of faultiness -- ranging from violations
of common sense and ambiguous statements to mathematical contradictions and
more. We evaluate a broad spectrum of LLMs, including open-source,
closed-source, and math-specialized models, using FaultyMath across three
dimensions: (i) How accurately can the models detect faulty math problems
without being explicitly prompted to do so? (ii) When provided with hints --
either correct or misleading -- about the validity of the problems, to what
extent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy
are the explanations generated by LLMs when they recognize a math problem as
flawed? Through extensive experimentation and detailed analysis, our results
demonstrate that existing LLMs largely function as Blind Solver and fall short
of the reasoning capabilities required to perform as Logical Thinker.
",2024-10-24T17:10:39Z
Optimizing Edge Offloading Decisions for Object Detection,"Jiaming Qiu, Ruiqi Wang, Brooks Hu, Roch Guerin, Chenyang Lu","  Recent advances in machine learning and hardware have produced embedded
devices capable of performing real-time object detection with commendable
accuracy. We consider a scenario in which embedded devices rely on an onboard
object detector, but have the option to offload detection to a more powerful
edge server when local accuracy is deemed too low. Resource constraints,
however, limit the number of images that can be offloaded to the edge. Our goal
is to identify which images to offload to maximize overall detection accuracy
under those constraints. To that end, the paper introduces a reward metric
designed to quantify potential accuracy improvements from offloading individual
images, and proposes an efficient approach to make offloading decisions by
estimating this reward based only on local detection results. The approach is
computationally frugal enough to run on embedded devices, and empirical
findings indicate that it outperforms existing alternatives in improving
detection accuracy even when the fraction of offloaded images is small.
",2024-10-24T17:09:37Z
"MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete
  Data","Muralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri","  Causal discovery in real-world systems, such as biological networks, is often
complicated by feedback loops and incomplete data. Standard algorithms, which
assume acyclic structures or fully observed data, struggle with these
challenges. To address this gap, we propose MissNODAG, a differentiable
framework for learning both the underlying cyclic causal graph and the
missingness mechanism from partially observed data, including data missing not
at random. Our framework integrates an additive noise model with an
expectation-maximization procedure, alternating between imputing missing values
and optimizing the observed data likelihood, to uncover both the cyclic
structures and the missingness mechanism. We demonstrate the effectiveness of
MissNODAG through synthetic experiments and an application to real-world gene
perturbation data.
",2024-10-24T17:09:10Z
"Using Parametric PINNs for Predicting Internal and External Turbulent
  Flows","Shinjan Ghosh, Amit Chakraborty, Georgia Olympia Brikis, Biswadip Dey","  Computational fluid dynamics (CFD) solvers employing two-equation eddy
viscosity models are the industry standard for simulating turbulent flows using
the Reynolds-averaged Navier-Stokes (RANS) formulation. While these methods are
computationally less expensive than direct numerical simulations, they can
still incur significant computational costs to achieve the desired accuracy. In
this context, physics-informed neural networks (PINNs) offer a promising
approach for developing parametric surrogate models that leverage both
existing, but limited CFD solutions and the governing differential equations to
predict simulation outcomes in a computationally efficient, differentiable, and
near real-time manner. In this work, we build upon the previously proposed
RANS-PINN framework, which only focused on predicting flow over a cylinder. To
investigate the efficacy of RANS-PINN as a viable approach to building
parametric surrogate models, we investigate its accuracy in predicting relevant
turbulent flow variables for both internal and external flows. To ensure
training convergence with a more complex loss function, we adopt a novel
sampling approach that exploits the domain geometry to ensure a proper balance
among the contributions from various regions within the solution domain. The
effectiveness of this framework is then demonstrated for two scenarios that
represent a broad class of internal and external flow problems.
",2024-10-24T17:08:20Z
Testing Support Size More Efficiently Than Learning Histograms,"Renato Ferreira Pinto Jr., Nathaniel Harms","  Consider two problems about an unknown probability distribution $p$:
  1. How many samples from $p$ are required to test if $p$ is supported on $n$
elements or not? Specifically, given samples from $p$, determine whether it is
supported on at most $n$ elements, or it is ""$\epsilon$-far"" (in total
variation distance) from being supported on $n$ elements.
  2. Given $m$ samples from $p$, what is the largest lower bound on its support
size that we can produce?
  The best known upper bound for problem (1) uses a general algorithm for
learning the histogram of the distribution $p$, which requires
$\Theta(\tfrac{n}{\epsilon^2 \log n})$ samples. We show that testing can be
done more efficiently than learning the histogram, using only
$O(\tfrac{n}{\epsilon \log n} \log(1/\epsilon))$ samples, nearly matching the
best known lower bound of $\Omega(\tfrac{n}{\epsilon \log n})$. This algorithm
also provides a better solution to problem (2), producing larger lower bounds
on support size than what follows from previous work. The proof relies on an
analysis of Chebyshev polynomial approximations outside the range where they
are designed to be good approximations, and the paper is intended as an
accessible self-contained exposition of the Chebyshev polynomial method.
",2024-10-24T17:05:34Z
Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling,"Mingtong Zhang, Kaifeng Zhang, Yunzhu Li","  Videos of robots interacting with objects encode rich information about the
objects' dynamics. However, existing video prediction approaches typically do
not explicitly account for the 3D information from videos, such as robot
actions and objects' 3D states, limiting their use in real-world robotic
applications. In this work, we introduce a framework to learn object dynamics
directly from multi-view RGB videos by explicitly considering the robot's
action trajectories and their effects on scene dynamics. We utilize the 3D
Gaussian representation of 3D Gaussian Splatting (3DGS) to train a
particle-based dynamics model using Graph Neural Networks. This model operates
on sparse control particles downsampled from the densely tracked 3D Gaussian
reconstructions. By learning the neural dynamics model on offline robot
interaction data, our method can predict object motions under varying initial
configurations and unseen robot actions. The 3D transformations of Gaussians
can be interpolated from the motions of control particles, enabling the
rendering of predicted future object states and achieving action-conditioned
video prediction. The dynamics model can also be applied to model-based
planning frameworks for object manipulation tasks. We conduct experiments on
various kinds of deformable materials, including ropes, clothes, and stuffed
animals, demonstrating our framework's ability to model complex shapes and
dynamics. Our project page is available at https://gs-dynamics.github.io.
",2024-10-24T17:02:52Z
"SkillMimicGen: Automated Demonstration Generation for Efficient Skill
  Learning and Deployment","Caelan Garrett, Ajay Mandlekar, Bowen Wen, Dieter Fox","  Imitation learning from human demonstrations is an effective paradigm for
robot manipulation, but acquiring large datasets is costly and
resource-intensive, especially for long-horizon tasks. To address this issue,
we propose SkillMimicGen (SkillGen), an automated system for generating
demonstration datasets from a few human demos. SkillGen segments human demos
into manipulation skills, adapts these skills to new contexts, and stitches
them together through free-space transit and transfer motion. We also propose a
Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and
termination components from SkillGen datasets, enabling skills to be sequenced
using motion planning at test-time. We demonstrate that SkillGen greatly
improves data generation and policy learning performance over a
state-of-the-art data generation framework, resulting in the capability to
produce data for large scene variations, including clutter, and agents that are
on average 24% more successful. We demonstrate the efficacy of SkillGen by
generating over 24K demonstrations across 18 task variants in simulation from
just 60 human demonstrations, and training proficient, often near-perfect, HSP
agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also
demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.
Videos, and more at https://skillgen.github.io.
",2024-10-24T16:59:26Z
PRISM: A Methodology for Auditing Biases in Large Language Models,"Leif Azzopardi, Yashar Moshfeghi","  Auditing Large Language Models (LLMs) to discover their biases and
preferences is an emerging challenge in creating Responsible Artificial
Intelligence (AI). While various methods have been proposed to elicit the
preferences of such models, countermeasures have been taken by LLM trainers,
such that LLMs hide, obfuscate or point blank refuse to disclosure their
positions on certain subjects. This paper presents PRISM, a flexible,
inquiry-based methodology for auditing LLMs - that seeks to illicit such
positions indirectly through task-based inquiry prompting rather than direct
inquiry of said preferences. To demonstrate the utility of the methodology, we
applied PRISM on the Political Compass Test, where we assessed the political
leanings of twenty-one LLMs from seven providers. We show LLMs, by default,
espouse positions that are economically left and socially liberal (consistent
with prior work). We also show the space of positions that these models are
willing to espouse - where some models are more constrained and less compliant
than others - while others are more neutral and objective. In sum, PRISM can
more reliably probe and audit LLMs to understand their preferences, biases and
constraints.
",2024-10-24T16:57:20Z
"Modulated Adaptive Fourier Neural Operators for Temporal Interpolation
  of Weather Forecasts","Jussi Leinonen, Boris Bonev, Thorsten Kurth, Yair Cohen","  Weather and climate data are often available at limited temporal resolution,
either due to storage limitations, or in the case of weather forecast models
based on deep learning, their inherently long time steps. The coarse temporal
resolution makes it difficult to capture rapidly evolving weather events. To
address this limitation, we introduce an interpolation model that reconstructs
the atmospheric state between two points in time for which the state is known.
The model makes use of a novel network layer that modifies the adaptive Fourier
neural operator (AFNO), which has been previously used in weather prediction
and other applications of machine learning to physics problems. The modulated
AFNO (ModAFNO) layer takes an embedding, here computed from the interpolation
target time, as an additional input and applies a learned shift-scale operation
inside the AFNO layers to adapt them to the target time. Thus, one model can be
used to produce all intermediate time steps. Trained to interpolate between two
time steps 6 h apart, the ModAFNO-based interpolation model produces 1 h
resolution intermediate time steps that are visually nearly indistinguishable
from the actual corresponding 1 h resolution data. The model reduces the RMSE
loss of reconstructing the intermediate steps by approximately 50% compared to
linear interpolation. We also demonstrate its ability to reproduce the
statistics of extreme weather events such as hurricanes and heat waves better
than 6 h resolution data. The ModAFNO layer is generic and is expected to be
applicable to other problems, including weather forecasting with tunable lead
time.
",2024-10-24T16:48:32Z
LLMs for Extremely Low-Resource Finno-Ugric Languages,"Taido Purason, Hele-Andra Kuulmets, Mark Fishel","  The advancement of large language models (LLMs) has predominantly focused on
high-resource languages, leaving low-resource languages, such as those in the
Finno-Ugric family, significantly underrepresented. This paper addresses this
gap by focusing on V\~oro, Livonian, and Komi. We cover almost the entire cycle
of LLM creation, from data collection to instruction tuning and evaluation. Our
contributions include developing multilingual base and instruction-tuned
models; creating evaluation benchmarks, including the smugri-MT-bench
multi-turn conversational benchmark; and conducting human evaluation. We intend
for this work to promote linguistic diversity, ensuring that lesser-resourced
languages can benefit from advancements in NLP.
",2024-10-24T16:48:12Z
"Comparative Analysis of Indicators for Multiobjective Diversity
  Optimization","Ksenia Pereverdieva, André Deutz, Tessa Ezendam, Thomas Bäck, Hèrm Hofmeyer, Michael T. M. Emmerich","  Indicator-based (multiobjective) diversity optimization aims at finding a set
of near (Pareto-)optimal solutions that maximizes a diversity indicator, where
diversity is typically interpreted as the number of essentially different
solutions. Whereas, in the first diversity-oriented evolutionary multiobjective
optimization algorithm, the NOAH algorithm by Ulrich and Thiele, the Solow
Polasky Diversity (also related to Magnitude) served as a metric, other
diversity indicators might be considered, such as the parameter-free Max-Min
Diversity, and the Riesz s-Energy, which features uniformly distributed
solution sets. In this paper, focusing on multiobjective diversity
optimization, we discuss different diversity indicators from the perspective of
indicator-based evolutionary algorithms (IBEA) with multiple objectives. We
examine theoretical, computational, and practical properties of these
indicators, such as monotonicity in species, twinning, monotonicity in
distance, strict monotonicity in distance, uniformity of maximizing point sets,
computational effort for a set of size~n, single-point contributions, subset
selection, and submodularity. We present new theorems -- including a proof of
the NP-hardness of the Riesz s-Energy Subset Selection Problem -- and
consolidate existing results from the literature. In the second part, we apply
these indicators in the NOAH algorithm and analyze search dynamics through an
example. We examine how optimizing with one indicator affects the performance
of others and propose NOAH adaptations specific to the Max-Min indicator.
",2024-10-24T16:40:36Z
"ArterialNet: Reconstructing Arterial Blood Pressure Waveform with
  Wearable Pulsatile Signals, a Cohort-Aware Approach","Sicong Huang, Roozbeh Jafari, Bobak J. Mortazavi","  Continuous arterial blood pressure (ABP) monitoring is invasive but essential
for hemodynamic monitoring. Recent techniques have reconstructed ABP
non-invasively using pulsatile signals but produced inaccurate systolic and
diastolic blood pressure (SBP and DBP) values and were sensitive to individual
variability. ArterialNet integrates generalized pulsatile-to-ABP signal
translation and personalized feature extraction using hybrid loss functions and
regularization. We validated ArterialNet using the MIMIC-III dataset and
achieved a root mean square error (RMSE) of 5.41 mmHg, with at least a 58%
lower standard deviation. ArterialNet reconstructed ABP with an RMSE of 7.99
mmHg in remote health scenarios. ArterialNet achieved superior performance in
ABP reconstruction and SBP and DBP estimations, with significantly reduced
subject variance, demonstrating its potential in remote health settings. We
also ablated ArterialNet architecture to investigate the contributions of each
component and evaluated its translational impact and robustness by conducting a
series of ablations on data quality and availability.
",2024-10-24T16:35:23Z
Meta-Learning with Heterogeneous Tasks,"Zhaofeng Si, Shu Hu, Kaiyi Ji, Siwei Lyu","  Meta-learning is a general approach to equip machine learning models with the
ability to handle few-shot scenarios when dealing with many tasks. Most
existing meta-learning methods work based on the assumption that all tasks are
of equal importance. However, real-world applications often present
heterogeneous tasks characterized by varying difficulty levels, noise in
training samples, or being distinctively different from most other tasks. In
this paper, we introduce a novel meta-learning method designed to effectively
manage such heterogeneous tasks by employing rank-based task-level learning
objectives, Heterogeneous Tasks Robust Meta-learning (HeTRoM). HeTRoM is
proficient in handling heterogeneous tasks, and it prevents easy tasks from
overwhelming the meta-learner. The approach allows for an efficient iterative
optimization algorithm based on bi-level optimization, which is then improved
by integrating statistical guidance. Our experimental results demonstrate that
our method provides flexibility, enabling users to adapt to diverse task
settings and enhancing the meta-learner's overall performance.
",2024-10-24T16:32:23Z
Creating and Repairing Robot Programs in Open-World Domains,"Claire Schlesinger, Arjun Guha, Joydeep Biswas","  Using Large Language Models (LLMs) to produce robot programs from natural
language has allowed for robot systems that can complete a higher diversity of
tasks. However, LLM-generated programs may be faulty, either due to ambiguity
in instructions, misinterpretation of the desired task, or missing information
about the world state. As these programs run, the state of the world changes
and they gather new information. When a failure occurs, it is important that
they recover from the current world state and avoid repeating steps that they
they previously completed successfully. We propose RoboRepair, a system which
traces the execution of a program up until error, and then runs an LLM-produced
recovery program that minimizes repeated actions.
  To evaluate the efficacy of our system, we create a benchmark consisting of
eleven tasks with various error conditions that require the generation of a
recovery program. We compare the efficiency of the recovery program to a plan
built with an oracle that has foreknowledge of future errors.
",2024-10-24T16:30:14Z
"Improving Small-Scale Large Language Models Function Calling for
  Reasoning Tasks","Graziano A. Manduzio, Federico A. Galatolo, Mario G. C. A. Cimino, Enzo Pasquale Scilingo, Lorenzo Cominelli","  Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional capabilities in natural language understanding and generation.
While these models excel in general complex reasoning tasks, they still face
challenges in mathematical problem-solving and logical reasoning. To address
these limitations, researchers have explored function calling abilities,
allowing LLMs to execute provided functions and utilize their outputs for task
completion. However, concentrating on specific tasks can be very inefficient
for large-scale LLMs to be used, because of the expensive cost of training and
inference stages they need in terms of computational resources. This study
introduces a novel framework for training smaller language models in function
calling, focusing on specific logical and mathematical reasoning tasks. The
approach aims to improve performances of small-scale models for these tasks
using function calling, ensuring a high level of accuracy. Our framework
employs an agent that, given a problem and a set of callable functions, queries
the LLM by injecting a description and examples of the usable functions into
the prompt and managing their calls in a step-by-step reasoning chain. This
process is used to create a dataset of correct and incorrect reasoning chain
chat completions from a large-scale LLM. This dataset is used to train a
smaller LLM using Reinforcement Learning from Human Feedback (RLHF),
specifically employing the Direct Preference Optimization (DPO) technique.
Experimental results demonstrate how the proposed approach balances the
trade-off between model size and performance, improving the ability of function
calling for reasoning tasks, in smaller models.
",2024-10-24T16:27:35Z
"Are LLMs Better than Reported? Detecting Label Errors and Mitigating
  Their Effect on Model Performance","Omer Nahum, Nitay Calderon, Orgad Keller, Idan Szpektor, Roi Reichart","  NLP benchmarks rely on standardized datasets for training and evaluating
models and are crucial for advancing the field. Traditionally, expert
annotations ensure high-quality labels; however, the cost of expert annotation
does not scale well with the growing demand for larger datasets required by
modern models. While crowd-sourcing provides a more scalable solution, it often
comes at the expense of annotation precision and consistency. Recent
advancements in large language models (LLMs) offer new opportunities to enhance
the annotation process, particularly for detecting label errors in existing
datasets. In this work, we consider the recent approach of LLM-as-a-judge,
leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through
a case study of four datasets from the TRUE benchmark, covering different tasks
and domains, we empirically analyze the labeling quality of existing datasets,
and compare expert, crowd-sourced, and our LLM-based annotations in terms of
agreement, label quality, and efficiency, demonstrating the strengths and
limitations of each annotation method. Our findings reveal a substantial number
of label errors, which, when corrected, induce a significant upward shift in
reported model performance. This suggests that many of the LLMs so-called
mistakes are due to label errors rather than genuine model failures.
Additionally, we discuss the implications of mislabeled data and propose
methods to mitigate them in training to improve model performance.
",2024-10-24T16:27:03Z
"Existence of solutions to port-Hamiltonian systems: initial value
  problems and optimal control","Willem Esterhuizen, Bernhard Maschke, Till Preuster, Manuel Schaller, Karl Worthmann","  We investigate the existence of solutions of reversible and irreversible
port-Hamiltonian systems. To this end, we utilize the associated exergy, a
function that is composed of the system's Hamiltonian and entropy, to prove
global existence in time for bounded control functions. The results are then
leveraged to prove existence of solutions of energy- and entropy-optimal
control problems. Last, we explore model predictive control tailored to
irreversible port-Hamiltonian systems by means of a numerical case study with a
heat exchanger network.
",2024-10-24T16:25:58Z
"Connectivity Labeling Schemes for Edge and Vertex Faults via Expander
  Hierarchies","Yaowei Long, Seth Pettie, Thatchaphol Saranurak","  We consider the problem of assigning short labels to the vertices and edges
of a graph $G$ so that given any query $\langle s,t,F\rangle$ with $|F|\leq f$,
we can determine whether $s$ and $t$ are still connected in $G-F$, given only
the labels of $F\cup\{s,t\}$. This problem has been considered when $F\subset
E$ (edge faults), where correctness is guaranteed with high probability
(w.h.p.) or deterministically, and when $F\subset V$ (vertex faults), both
w.h.p.~and deterministically. Our main results are as follows.
  [Deterministic Edge Faults.] We give a new deterministic labeling scheme for
edge faults that uses $\tilde{O}(\sqrt{f})$-bit labels, which can be
constructed in polynomial time. This improves on Dory and Parter's [PODC 2021]
existential bound of $O(f\log n)$ (requiring exponential time to compute) and
the efficient $\tilde{O}(f^2)$-bit scheme of Izumi, Emek, Wadayama, and
Masuzawa [PODC 2023]. Our construction uses an improved edge-expander hierarchy
and a distributed coding technique based on Reed-Solomon codes.
  [Deterministic Vertex Faults.] We improve Parter, Petruschka, and Pettie's
[STOC 2024] deterministic $O(f^7\log^{13} n)$-bit labeling scheme for vertex
faults to $O(f^4\log^{7.5} n)$ bits, using an improved vertex-expander
hierarchy and better sparsification of shortcut graphs.
  [Randomized Edge/Verex Faults.] We improve the size of Dory and Parter's
[PODC 2021] randomized edge fault labeling scheme from $O(\min\{f+\log n,
\log^3 n\})$ bits to $O(\min\{f+\log n, \log^2 n\log f\})$ bits, shaving a
$\log n/\log f$ factor. We also improve the size of Parter, Petruschka, and
Pettie's [STOC 2024] randomized vertex fault labeling scheme from $O(f^3\log^5
n)$ bits to $O(f^2\log^6 n)$ bits, which comes closer to their $\Omega(f)$-bit
lower bound.
",2024-10-24T16:20:57Z
A Survey of Multimodal Sarcasm Detection,"Shafkat Farabi, Tharindu Ranasinghe, Diptesh Kanojia, Yu Kong, Marcos Zampieri","  Sarcasm is a rhetorical device that is used to convey the opposite of the
literal meaning of an utterance. Sarcasm is widely used on social media and
other forms of computer-mediated communication motivating the use of
computational models to identify it automatically. While the clear majority of
approaches to sarcasm detection have been carried out on text only, sarcasm
detection often requires additional information present in tonality, facial
expression, and contextual images. This has led to the introduction of
multimodal models, opening the possibility to detect sarcasm in multiple
modalities such as audio, images, text, and video. In this paper, we present
the first comprehensive survey on multimodal sarcasm detection - henceforth MSD
- to date. We survey papers published between 2018 and 2023 on the topic, and
discuss the models and datasets used for this task. We also present future
research directions in MSD.
",2024-10-24T16:17:47Z
"Diff-Instruct++: Training One-step Text-to-image Generator Model to
  Align with Human Preferences",Weijian Luo,"  One-step text-to-image generator models offer advantages such as swift
inference efficiency, flexible architectures, and state-of-the-art generation
performance. In this paper, we study the problem of aligning one-step generator
models with human preferences for the first time. Inspired by the success of
reinforcement learning using human feedback (RLHF), we formulate the alignment
problem as maximizing expected human reward functions while adding an Integral
Kullback-Leibler divergence term to prevent the generator from diverging. By
overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the
first, fast-converging and image data-free human preference alignment method
for one-step text-to-image generators. We also introduce novel theoretical
insights, showing that using CFG for diffusion distillation is secretly doing
RLHF with DI++. Such an interesting finding brings understanding and potential
contributions to future research involving CFG. In the experiment sections, we
align both UNet-based and DiT-based one-step generators using DI++, which use
the Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusion
processes. The resulting DiT-based one-step text-to-image model achieves a
strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO
validation prompt dataset. It also achieves a leading Human preference Score
(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable
Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoretical
contributions and empirical evidence indicate that DI++ is a strong
human-preference alignment approach for one-step text-to-image models.
",2024-10-24T16:17:18Z
"Multi-Class Abnormality Classification in Video Capsule Endoscopy Using
  Deep Learning","Arnav Samal,  Ranya","  This report outlines Team Seq2Cure's deep learning approach for the Capsule
Vision 2024 Challenge, leveraging an ensemble of convolutional neural networks
(CNNs) and transformer-based architectures for multi-class abnormality
classification in video capsule endoscopy frames. The dataset comprised over
50,000 frames from three public sources and one private dataset, labeled across
10 abnormality classes. To overcome the limitations of traditional CNNs in
capturing global context, we integrated CNN and transformer models within a
multi-model ensemble. Our approach achieved a balanced accuracy of 86.34
percent and a mean AUC-ROC score of 0.9908 on the validation set, with
significant improvements in classifying complex abnormalities. Code is
available at http://github.com/arnavs04/capsule-vision-2024 .
",2024-10-24T16:13:06Z
Packing Short Cycles,"Matthias Bentert, Fedor V. Fomin, Petr A. Golovach, Tuukka Korhonen, William Lochet, Fahad Panolan, M. S. Ramanujan, Saket Saurabh, Kirill Simonov","  Cycle packing is a fundamental problem in optimization, graph theory, and
algorithms. Motivated by recent advancements in finding vertex-disjoint paths
between a specified set of vertices that either minimize the total length of
the paths [Bj\""orklund, Husfeldt, ICALP 2014; Mari, Mukherjee, Pilipczuk, and
Sankowski, SODA 2024] or request the paths to be shortest [Lochet, SODA 2021],
we consider the following cycle packing problems: Min-Sum Cycle Packing and
Shortest Cycle Packing.
  In Min-Sum Cycle Packing, we try to find, in a weighted undirected graph, $k$
vertex-disjoint cycles of minimum total weight. Our first main result is an
algorithm that, for any fixed $k$, solves the problem in polynomial time. We
complement this result by establishing the W[1]-hardness of Min-Sum Cycle
Packing parameterized by $k$. The same results hold for the version of the
problem where the task is to find $k$ edge-disjoint cycles.
  Our second main result concerns Shortest Cycle Packing, which is a special
case of Min-Sum Cycle Packing that asks to find a packing of $k$ shortest
cycles in a graph. We prove this problem to be fixed-parameter tractable (FPT)
when parameterized by $k$ on weighted planar graphs. We also obtain a
polynomial kernel for the edge-disjoint variant of the problem on planar
graphs. Deciding whether Min-Sum Cycle Packing is FPT on planar graphs and
whether Shortest Cycle Packing is FPT on general graphs remain challenging open
questions.
",2024-10-24T16:08:41Z
"Guiding Empowerment Model: Liberating Neurodiversity in Online Higher
  Education","Hannah Beaux, Pegah Karimi, Otilia Pop, Rob Clark","  In this innovative practice full paper, we address the equity gap for
neurodivergent and situationally limited learners by identifying the spectrum
of dynamic factors that impact learning and function. Educators have shown a
growing interest in identifying learners' cognitive abilities and learning
preferences to measure their impact on academic achievement. Often institutions
employ one-size-fits-all approaches leaving the burden on disabled students to
self-advocate or tolerate inadequate support. Emerging frameworks guide
neurodivergent learners through instructional approaches, such as online
education. However, these frameworks fail to address holistic environmental
needs or recommend technology interventions, particularly for those with
undisclosed learning or developmental disabilities and situational limitations.
In this article, we integrate a neurodivergent perspective through secondary
research of around 100 articles to introduce a Guiding Empowerment Model
involving key cognitive and situational factors that contextualize day-to-day
experiences affecting learner ability. We synthesize three sample student
profiles that highlight user problems in functioning. We use this model to
evaluate sample learning platform features and other supportive technology
solutions. The proposed approach augments frameworks such as Universal Design
for Learning to consider factors including various sensory processing
differences, social connection challenges, and environmental limitations. We
suggest that by applying the mode through technology-enabled features such as
customizable task management, guided varied content access, and guided
multi-modal collaboration, major learning barriers of neurodivergent and
situationally limited learners will be removed to activate the successful
pursuit of their academic goals.
",2024-10-24T16:05:38Z
Exploring the Universe with SNAD: Anomaly Detection in Astronomy,"Alina A. Volnova, Patrick D. Aleo, Anastasia Lavrukhina, Etienne Russeil, Timofey Semenikhin, Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Vladimir Korolev, Konstantin Malanchev, Maria V. Pruzhinskaya, Sreevarsha Sreejith","  SNAD is an international project with a primary focus on detecting
astronomical anomalies within large-scale surveys, using active learning and
other machine learning algorithms. The work carried out by SNAD not only
contributes to the discovery and classification of various astronomical
phenomena but also enhances our understanding and implementation of machine
learning techniques within the field of astrophysics. This paper provides a
review of the SNAD project and summarizes the advancements and achievements
made by the team over several years.
",2024-10-24T16:05:11Z
"Learning Collusion in Episodic, Inventory-Constrained Markets","Paul Friedrich, Barna Pásztor, Giorgia Ramponi","  Pricing algorithms have demonstrated the capability to learn tacit collusion
that is largely unaddressed by current regulations. Their increasing use in
markets, including oligopolistic industries with a history of collusion, calls
for closer examination by competition authorities. In this paper, we extend the
study of tacit collusion in learning algorithms from basic pricing games to
more complex markets characterized by perishable goods with fixed supply and
sell-by dates, such as airline tickets, perishables, and hotel rooms. We
formalize collusion within this framework and introduce a metric based on price
levels under both the competitive (Nash) equilibrium and collusive
(monopolistic) optimum. Since no analytical expressions for these price levels
exist, we propose an efficient computational approach to derive them. Through
experiments, we demonstrate that deep reinforcement learning agents can learn
to collude in this more complex domain. Additionally, we analyze the underlying
mechanisms and structures of the collusive strategies these agents adopt.
",2024-10-24T15:58:14Z
End-to-end Training for Recommendation with Language-based User Profiles,"Zhaolin Gao, Joyce Zhou, Yijia Dai, Thorsten Joachims","  Many online platforms maintain user profiles for personalization.
Unfortunately, these profiles are typically not interpretable or easily
modifiable by the user. To remedy this shortcoming, we explore natural
language-based user profiles, as they promise enhanced transparency and
scrutability of recommender systems. While existing work has shown that
language-based profiles from standard LLMs can be effective, such generalist
LLMs are unlikely to be optimal for this task. In this paper, we introduce
LangPTune, the first end-to-end learning method for training LLMs to produce
language-based user profiles that optimize recommendation effectiveness.
Through comprehensive evaluations of LangPTune across various training
configurations and benchmarks, we demonstrate that our approach significantly
outperforms existing profile-based methods. In addition, it approaches
performance levels comparable to state-of-the-art, less transparent recommender
systems, providing a robust and interpretable alternative to conventional
systems. Finally, we validate the relative interpretability of these
language-based user profiles through user studies involving crowdworkers and
GPT-4-based evaluations. Implementation of LangPTune can be found at
https://github.com/ZhaolinGao/LangPTune.
",2024-10-24T15:57:17Z
A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics,"Katharina Friedl, Noémie Jaquier, Jens Lundell, Tamim Asfour, Danica Kragic","  By incorporating physical consistency as inductive bias, deep neural networks
display increased generalization capabilities and data efficiency in learning
nonlinear dynamic models. However, the complexity of these models generally
increases with the system dimensionality, requiring larger datasets, more
complex deep networks, and significant computational effort. We propose a novel
geometric network architecture to learn physically-consistent reduced-order
dynamic parameters that accurately describe the original high-dimensional
system behavior. This is achieved by building on recent advances in model-order
reduction and by adopting a Riemannian perspective to jointly learn a
structure-preserving latent space and the associated low-dimensional dynamics.
Our approach enables accurate long-term predictions of the high-dimensional
dynamics of rigid and deformable systems with increased data efficiency by
inferring interpretable and physically plausible reduced Lagrangian models.
",2024-10-24T15:53:21Z
"The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models
  and Detection Methods","Linda Laurier, Ave Giulietta, Arlo Octavia, Meade Cleti","  The emergence of diffusion models has transformed synthetic media generation,
offering unmatched realism and control over content creation. These
advancements have driven innovation across fields such as art, design, and
scientific visualization. However, they also introduce significant ethical and
societal challenges, particularly through the creation of hyper-realistic
images that can facilitate deepfakes, misinformation, and unauthorized
reproduction of copyrighted material. In response, the need for effective
detection mechanisms has become increasingly urgent. This review examines the
evolving adversarial relationship between diffusion model development and the
advancement of detection methods. We present a thorough analysis of
contemporary detection strategies, including frequency and spatial domain
techniques, deep learning-based approaches, and hybrid models that combine
multiple methodologies. We also highlight the importance of diverse datasets
and standardized evaluation metrics in improving detection accuracy and
generalizability. Our discussion explores the practical applications of these
detection systems in copyright protection, misinformation prevention, and
forensic analysis, while also addressing the ethical implications of synthetic
media. Finally, we identify key research gaps and propose future directions to
enhance the robustness and adaptability of detection methods in line with the
rapid advancements of diffusion models. This review emphasizes the necessity of
a comprehensive approach to mitigating the risks associated with AI-generated
content in an increasingly digital world.
",2024-10-24T15:51:04Z
"Omics-driven hybrid dynamic modeling of bioprocesses with uncertainty
  estimation","Sebastián Espinel-Ríos, José Montaño López, José L. Avalos","  This work presents an omics-driven modeling pipeline that integrates
machine-learning tools to facilitate the dynamic modeling of multiscale
biological systems. Random forests and permutation feature importance are
proposed to mine omics datasets, guiding feature selection and dimensionality
reduction for dynamic modeling. Continuous and differentiable machine-learning
functions can be trained to link the reduced omics feature set to key
components of the dynamic model, resulting in a hybrid model. As proof of
concept, we apply this framework to a high-dimensional proteomics dataset of
$\textit{Saccharomyces cerevisiae}$. After identifying key intracellular
proteins that correlate with cell growth, targeted dynamic experiments are
designed, and key model parameters are captured as functions of the selected
proteins using Gaussian processes. This approach captures the dynamic behavior
of yeast strains under varying proteome profiles while estimating the
uncertainty in the hybrid model's predictions. The outlined modeling framework
is adaptable to other scenarios, such as integrating additional layers of omics
data for more advanced multiscale biological systems, or employing alternative
machine-learning methods to handle larger datasets. Overall, this study
outlines a strategy for leveraging omics data to inform multiscale dynamic
modeling in systems biology and bioprocess engineering.
",2024-10-24T15:50:35Z
"FedSPD: A Soft-clustering Approach for Personalized Decentralized
  Federated Learning","I-Cheng Lin, Osman Yagan, Carlee Joe-Wong","  Federated learning has recently gained popularity as a framework for
distributed clients to collaboratively train a machine learning model using
local data. While traditional federated learning relies on a central server for
model aggregation, recent advancements adopt a decentralized framework,
enabling direct model exchange between clients and eliminating the single point
of failure. However, existing decentralized frameworks often assume all clients
train a shared model. Personalizing each client's model can enhance
performance, especially with heterogeneous client data distributions. We
propose FedSPD, an efficient personalized federated learning algorithm for the
decentralized setting, and show that it learns accurate models even in
low-connectivity networks. To provide theoretical guarantees on convergence, we
introduce a clustering-based framework that enables consensus on models for
distinct data clusters while personalizing to unique mixtures of these clusters
at different clients. This flexibility, allowing selective model updates based
on data distribution, substantially reduces communication costs compared to
prior work on personalized federated learning in decentralized settings.
Experimental results on real-world datasets show that FedSPD outperforms
multiple decentralized variants of personalized federated learning algorithms,
especially in scenarios with low-connectivity networks.
",2024-10-24T15:48:34Z
Provably Robust Watermarks for Open-Source Language Models,"Miranda Christ, Sam Gunn, Tal Malkin, Mariana Raykova","  The recent explosion of high-quality language models has necessitated new
methods for identifying AI-generated text. Watermarking is a leading solution
and could prove to be an essential tool in the age of generative AI. Existing
approaches embed watermarks at inference and crucially rely on the large
language model (LLM) specification and parameters being secret, which makes
them inapplicable to the open-source setting. In this work, we introduce the
first watermarking scheme for open-source LLMs. Our scheme works by modifying
the parameters of the model, but the watermark can be detected from just the
outputs of the model. Perhaps surprisingly, we prove that our watermarks are
unremovable under certain assumptions about the adversary's knowledge. To
demonstrate the behavior of our construction under concrete parameter
instantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We
demonstrate robustness to both token substitution and perturbation of the model
parameters. We find that the stronger of these attacks, the model-perturbation
attack, requires deteriorating the quality score to 0 out of 100 in order to
bring the detection rate down to 50%.
",2024-10-24T15:44:34Z
"DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate
  Hallucinations","Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran","  Large Language Models (LLMs) often hallucinate, producing unfaithful or
factually incorrect outputs by misrepresenting the provided context or
incorrectly recalling internal knowledge. Recent studies have identified
specific attention heads within the Transformer architecture, known as
retrieval heads, responsible for extracting relevant contextual information. We
hypothesise that masking these retrieval heads can induce hallucinations and
that contrasting the outputs of the base LLM and the masked LLM can reduce
hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads
(DeCoRe), a novel training-free decoding strategy that amplifies information
found in the context and model parameters. DeCoRe mitigates potentially
hallucinated responses by dynamically contrasting the outputs of the base LLM
and the masked LLM, using conditional entropy as a guide. Our extensive
experiments confirm that DeCoRe significantly improves performance on tasks
requiring high contextual faithfulness, such as summarisation (XSum by 18.6%),
instruction following (MemoTrap by 10.9%), and open-book question answering
(NQ-Open by 2.4% and NQ-Swap by 5.5%).
",2024-10-24T15:44:33Z
"Bilinear Sequence Regression: A Model for Learning from Long Sequences
  of High-dimensional Tokens","Vittorio Erba, Emanuele Troiani, Luca Biggio, Antoine Maillard, Lenka Zdeborová","  Current progress in artificial intelligence is centered around so-called
large language models that consist of neural networks processing long sequences
of high-dimensional vectors called tokens. Statistical physics provides
powerful tools to study the functioning of learning with neural networks and
has played a recognized role in the development of modern machine learning. The
statistical physics approach relies on simplified and analytically tractable
models of data. However, simple tractable models for long sequences of
high-dimensional tokens are largely underexplored. Inspired by the crucial role
models such as the single-layer teacher-student perceptron (aka generalized
linear regression) played in the theory of fully connected neural networks, in
this paper, we introduce and study the bilinear sequence regression (BSR) as
one of the most basic models for sequences of tokens. We note that modern
architectures naturally subsume the BSR model due to the skip connections.
Building on recent methodological progress, we compute the Bayes-optimal
generalization error for the model in the limit of long sequences of
high-dimensional tokens, and provide a message-passing algorithm that matches
this performance. We quantify the improvement that optimal learning brings with
respect to vectorizing the sequence of tokens and learning via simple linear
regression. We also unveil surprising properties of the gradient descent
algorithms in the BSR model.
",2024-10-24T15:44:03Z
Probabilistic Language-Image Pre-Training,"Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun","  Vision-language models (VLMs) embed aligned image-text pairs into a joint
space but often rely on deterministic embeddings, assuming a one-to-one
correspondence between images and texts. This oversimplifies real-world
relationships, which are inherently many-to-many, with multiple captions
describing a single image and vice versa. We introduce Probabilistic
Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained
on a billion-scale image-text dataset using only probabilistic objectives,
achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot
accuracy with ViT-B/16). ProLIP efficiently estimates uncertainty by an
""uncertainty token"" without extra parameters. We also introduce a novel
inclusion loss that enforces distributional inclusion relationships between
image-text pairs and between original and masked inputs. Experiments
demonstrate that, by leveraging uncertainty estimates, ProLIP benefits
downstream tasks and aligns with intuitive notions of uncertainty, e.g.,
shorter texts being more uncertain and more general inputs including specific
ones. Utilizing text uncertainties, we further improve ImageNet accuracy from
74.6% to 75.8% (under a few-shot setting), supporting the practical advantages
of our probabilistic approach. The code is available at
https://github.com/naver-ai/prolip
",2024-10-24T15:42:25Z
Demystifying Large Language Models for Medicine: A Primer,"Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, Benjamin Hou, Maame Sarfo-Gyamfi, Gongbo Zhang, Aidan Gilson, Balu Bhasuran, Zhe He, Aidong Zhang, Jimeng Sun, Chunhua Weng, Ronald M. Summers, Qingyu Chen, Yifan Peng, Zhiyong Lu","  Large language models (LLMs) represent a transformative class of AI tools
capable of revolutionizing various aspects of healthcare by generating
human-like responses across diverse contexts and adapting to novel tasks
following human instructions. Their potential application spans a broad range
of medical tasks, such as clinical documentation, matching patients to clinical
trials, and answering medical questions. In this primer paper, we propose an
actionable guideline to help healthcare professionals more efficiently utilize
LLMs in their work, along with a set of best practices. This approach consists
of several main phases, including formulating the task, choosing LLMs, prompt
engineering, fine-tuning, and deployment. We start with the discussion of
critical considerations in identifying healthcare tasks that align with the
core capabilities of LLMs and selecting models based on the selected task and
data, performance requirements, and model interface. We then review the
strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs
to specialized medical tasks. Deployment considerations, including regulatory
compliance, ethical guidelines, and continuous monitoring for fairness and
bias, are also discussed. By providing a structured step-by-step methodology,
this tutorial aims to equip healthcare professionals with the tools necessary
to effectively integrate LLMs into clinical practice, ensuring that these
powerful technologies are applied in a safe, reliable, and impactful manner.
",2024-10-24T15:41:56Z
"DL-Polycube: Deep learning enhanced polycube method for high-quality
  hexahedral mesh generation and volumetric spline construction","Yuxuan Yu, Yuzhuo Fang, Hua Tong, Yongjie Jessica Zhang","  In this paper, we present a novel algorithm that integrates deep learning
with the polycube method (DL-Polycube) to generate high-quality hexahedral
(hex) meshes, which are then used to construct volumetric splines for
isogeometric analysis. Our DL-Polycube algorithm begins by establishing a
connection between surface triangular meshes and polycube structures. We employ
deep neural network to classify surface triangular meshes into their
corresponding polycube structures. Following this, we combine the acquired
polycube structural information with unsupervised learning to perform surface
segmentation of triangular meshes. This step addresses the issue of
segmentation not corresponding to a polycube while reducing manual
intervention. Quality hex meshes are then generated from the polycube
structures, with employing octree subdivision, parametric mapping and quality
improvement techniques. The incorporation of deep learning for creating
polycube structures, combined with unsupervised learning for segmentation of
surface triangular meshes, substantially accelerates hex mesh generation.
Finally, truncated hierarchical B-splines are constructed on the generated hex
meshes. We extract trivariate B\'ezier elements from these splines and apply
them directly in isogeometric analysis. We offer several examples to
demonstrate the robustness of our DL-Polycube algorithm.
",2024-10-24T15:35:08Z
Intention Is All You Need,Advait Sarkar,"  Among the many narratives of the transformative power of Generative AI is one
that sees in the world a latent nation of programmers who need to wield nothing
but intentions and natural language to render their ideas in software. In this
paper, this outlook is problematised in two ways. First, it is observed that
generative AI is not a neutral vehicle of intention. Multiple recent studies
paint a picture of the ""mechanised convergence"" phenomenon, namely, that
generative AI has a homogenising effect on intention. Second, it is observed
that the formation of intention itself is immensely challenging. Constraints,
materiality, and resistance can offer paths to design metaphors for intentional
tools. Finally, existentialist approaches to intention are discussed and
possible implications for programming are proposed in the form of a
speculative, illustrative set of intentional programming practices.
",2024-10-24T15:33:34Z
We Augmented Whisper With kNN and You Won't Believe What Came Next,"Maya K. Nachesa, Vlad Niculae","  Speech recognition performance varies by language, domain, and speaker
characteristics such as accent, and fine-tuning a model on any of these
categories may lead to catastrophic forgetting. $k$ nearest neighbor search
($k$NN), first proposed for neural sequence decoders for natural language
generation (NLG) and machine translation (MT), is a non-parametric method that
can instead adapt by building an external datastore that can then be searched
during inference time, without training the underlying model. We show that
Whisper, a transformer end-to-end speech model, benefits from $k$NN. We
investigate the differences between the speech and text setups. We discuss
implications for speaker adaptation, and analyze improvements by gender,
accent, and age.
",2024-10-24T15:32:52Z
"Expanding AI Awareness Through Everyday Interactions with AI: A
  Reflective Journal Study","Ashish Hingle, Aditya Johri","  As the application of AI continues to expand, students in technology programs
are poised to be both producers and users of the technologies. They are also
positioned to engage with AI applications within and outside the classroom.
While focusing on the curriculum when examining students' AI knowledge is
common, extending this connection to students' everyday interactions with AI
provides a more complete picture of their learning. In this paper, we explore
student's awareness and engagement with AI in the context of school and their
daily lives. Over six weeks, 22 undergraduate students participated in a
reflective journal study and submitted a weekly journal entry about their
interactions with AI. The participants were recruited from a technology and
society course that focuses on the implications of technology on people,
communities, and processes. In their weekly journal entries, participants
reflected on interactions with AI on campus (coursework, advertises campus
events, or seminars) and beyond (social media, news, or conversations with
friends and family). The journal prompts were designed to help them think
through what they had read, watched, or been told and reflect on the
development of their own perspectives, knowledge, and literacy on the topic.
Overall, students described nine categories of interactions: coursework, news
and current events, using software and applications, university events, social
media related to their work, personal discussions with friends and family,
interacting with content, and gaming. Students reported that completing the
diaries allowed them time for reflection and made them more aware of the
presence of AI in their daily lives and of its potential benefits and
drawbacks. This research contributes to the ongoing work on AI awareness and
literacy by bringing in perspectives from beyond a formal educational context.
",2024-10-24T15:26:34Z
"Learning to Explore with Lagrangians for Bandits under Unknown Linear
  Constraints","Udvas Das, Debabrota Basu","  Pure exploration in bandits models multiple real-world problems, such as
tuning hyper-parameters or conducting user studies, where different safety,
resource, and fairness constraints on the decision space naturally appear. We
study these problems as pure exploration in multi-armed bandits with unknown
linear constraints, where the aim is to identify an $r$$\textit{-good feasible
policy}$. First, we propose a Lagrangian relaxation of the sample complexity
lower bound for pure exploration under constraints. We show how this lower
bound evolves with the sequential estimation of constraints. Second, we
leverage the Lagrangian lower bound and the properties of convex optimisation
to propose two computationally efficient extensions of Track-and-Stop and
Gamified Explorer, namely LATS and LAGEX. To this end, we propose a
constraint-adaptive stopping rule, and while tracking the lower bound, use
pessimistic estimate of the feasible set at each step. We show that these
algorithms achieve asymptotically optimal sample complexity upper bounds up to
constraint-dependent constants. Finally, we conduct numerical experiments with
different reward distributions and constraints that validate efficient
performance of LAGEX and LATS with respect to baselines.
",2024-10-24T15:26:14Z
From Efficiency to Equity: Measuring Fairness in Preference Learning,"Shreeyash Gowaikar, Hugo Berard, Rashid Mushkani, Shin Koseki","  As AI systems, particularly generative models, increasingly influence
decision-making, ensuring that they are able to fairly represent diverse human
preferences becomes crucial. This paper introduces a novel framework for
evaluating epistemic fairness in preference learning models inspired by
economic theories of inequality and Rawlsian justice. We propose metrics
adapted from the Gini Coefficient, Atkinson Index, and Kuznets Ratio to
quantify fairness in these models. We validate our approach using two datasets:
a custom visual preference dataset (AI-EDI-Space) and the Jester Jokes dataset.
Our analysis reveals variations in model performance across users, highlighting
potential epistemic injustices. We explore pre-processing and in-processing
techniques to mitigate these inequalities, demonstrating a complex relationship
between model efficiency and fairness. This work contributes to AI ethics by
providing a framework for evaluating and improving epistemic fairness in
preference learning models, offering insights for developing more inclusive AI
systems in contexts where diverse human preferences are crucial.
",2024-10-24T15:25:56Z
"High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong
  Generalization and Scaling Laws","M. Emrullah Ildiz, Halil Alperen Gozeten, Ege Onur Taga, Marco Mondelli, Samet Oymak","  A growing number of machine learning scenarios rely on knowledge distillation
where one uses the output of a surrogate model as labels to supervise the
training of a target model. In this work, we provide a sharp characterization
of this process for ridgeless, high-dimensional regression, under two settings:
(i) model shift, where the surrogate model is arbitrary, and (ii) distribution
shift, where the surrogate model is the solution of empirical risk minimization
with out-of-distribution data. In both cases, we characterize the precise risk
of the target model through non-asymptotic bounds in terms of sample size and
data distribution under mild conditions. As a consequence, we identify the form
of the optimal surrogate model, which reveals the benefits and limitations of
discarding weak features in a data-dependent fashion. In the context of
weak-to-strong (W2S) generalization, this has the interpretation that (i) W2S
training, with the surrogate as the weak model, can provably outperform
training with strong labels under the same data budget, but (ii) it is unable
to improve the data scaling law. We validate our results on numerical
experiments both on ridgeless regression and on neural network architectures.
",2024-10-24T15:22:53Z
"From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers
  for Underrepresented Languages","Artur Kiulian, Anton Polishko, Mykola Khandoga, Yevhen Kostiuk, Guillermo Gabrielli, Łukasz Gagała, Fadi Zaraket, Qusai Abu Obaida, Hrishikesh Garud, Wendy Wing Yee Mak, Dmytro Chaplynskyi, Selma Belhadj Amor, Grigol Peradze","  In this paper, we propose a model-agnostic cost-effective approach to
developing bilingual base large language models (LLMs) to support English and
any target language. The method includes vocabulary expansion, initialization
of new embeddings, model training and evaluation. We performed our experiments
with three languages, each using a non-Latin script - Ukrainian, Arabic, and
Georgian.
  Our approach demonstrates improved language performance while reducing
computational costs. It mitigates the disproportionate penalization of
underrepresented languages, promoting fairness and minimizing adverse phenomena
such as code-switching and broken grammar. Additionally, we introduce new
metrics to evaluate language quality, revealing that vocabulary size
significantly impacts the quality of generated text.
",2024-10-24T15:20:54Z
Diffusion for Multi-Embodiment Grasping,"Roman Freiberg, Alexander Qualmann, Ngo Anh Vien, Gerhard Neumann","  Grasping is a fundamental skill in robotics with diverse applications across
medical, industrial, and domestic domains. However, current approaches for
predicting valid grasps are often tailored to specific grippers, limiting their
applicability when gripper designs change. To address this limitation, we
explore the transfer of grasping strategies between various gripper designs,
enabling the use of data from diverse sources. In this work, we present an
approach based on equivariant diffusion that facilitates gripper-agnostic
encoding of scenes containing graspable objects and gripper-aware decoding of
grasp poses by integrating gripper geometry into the model. We also develop a
dataset generation framework that produces cluttered scenes with variable-sized
object heaps, improving the training of grasp synthesis methods. Experimental
evaluation on diverse object datasets demonstrates the generalizability of our
approach across gripper architectures, ranging from simple parallel-jaw
grippers to humanoid hands, outperforming both single-gripper and multi-gripper
state-of-the-art methods.
",2024-10-24T15:20:16Z
"Highly efficient non-rigid registration in k-space with application to
  cardiac Magnetic Resonance Imaging","Aya Ghoul, Kerstin Hammernik, Andreas Lingg, Patrick Krumm, Daniel Rueckert, Sergios Gatidis, Thomas Küstner","  In Magnetic Resonance Imaging (MRI), high temporal-resolved motion can be
useful for image acquisition and reconstruction, MR-guided radiotherapy,
dynamic contrast-enhancement, flow and perfusion imaging, and functional
assessment of motion patterns in cardiovascular, abdominal, peristaltic, fetal,
or musculoskeletal imaging. Conventionally, these motion estimates are derived
through image-based registration, a particularly challenging task for complex
motion patterns and high dynamic resolution. The accelerated scans in such
applications result in imaging artifacts that compromise the motion estimation.
In this work, we propose a novel self-supervised deep learning-based framework,
dubbed the Local-All Pass Attention Network (LAPANet), for non-rigid motion
estimation directly from the acquired accelerated Fourier space, i.e. k-space.
The proposed approach models non-rigid motion as the cumulative sum of local
translational displacements, following the Local All-Pass (LAP) registration
technique. LAPANet was evaluated on cardiac motion estimation across various
sampling trajectories and acceleration rates. Our results demonstrate superior
accuracy compared to prior conventional and deep learning-based registration
methods, accommodating as few as 2 lines/frame in a Cartesian trajectory and 3
spokes/frame in a non-Cartesian trajectory. The achieved high temporal
resolution (less than 5 ms) for non-rigid motion opens new avenues for motion
detection, tracking and correction in dynamic and real-time MRI applications.
",2024-10-24T15:19:59Z
"MazeNet: An Accurate, Fast, and Scalable Deep Learning Solution for
  Steiner Minimum Trees","Gabriel Díaz Ramos, Toros Arikan, Richard G. Baraniuk","  The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem,
which seeks the shortest interconnection of a given number of terminals in a
rectilinear plane while avoiding obstacles, is a critical task in integrated
circuit design, network optimization, and robot path planning. Since OARSMT is
NP-hard, exact algorithms scale poorly with the number of terminals, leading
practical solvers to sacrifice accuracy for large problems. We propose MazeNet,
a deep learning-based method that learns to solve the OARSMT from data. MazeNet
reframes OARSMT as a maze-solving task that can be addressed with a recurrent
convolutional neural network (RCNN). A key hallmark of MazeNet is its
scalability: we only need to train the RCNN blocks on mazes with a small number
of terminals; larger mazes can be solved by replicating the same pre-trained
blocks to create a larger network. Across a wide range of experiments, MazeNet
achieves perfect OARSMT-solving accuracy, significantly reduces runtime
compared to classical exact algorithms, and can handle more terminals than
state-of-the-art approximate algorithms.
",2024-10-24T15:19:48Z
"Multi-Scale Diffusion: Enhancing Spatial Layout in High-Resolution
  Panoramic Image Generation","Xiaoyu Zhang, Teng Zhou, Xinlong Zhang, Jia Wei, Yongchuan Tang","  Diffusion models have recently gained recognition for generating diverse and
high-quality content, especially in the domain of image synthesis. These models
excel not only in creating fixed-size images but also in producing panoramic
images. However, existing methods often struggle with spatial layout
consistency when producing high-resolution panoramas, due to the lack of
guidance of the global image layout. In this paper, we introduce the
Multi-Scale Diffusion (MSD) framework, a plug-and-play module that extends the
existing panoramic image generation framework to multiple resolution levels. By
utilizing gradient descent techniques, our method effectively incorporates
structural information from low-resolution images into high-resolution outputs.
A comprehensive evaluation of the proposed method was conducted, comparing it
with the prior works in qualitative and quantitative dimensions. The evaluation
results demonstrate that our method significantly outperforms others in
generating coherent high-resolution panoramas.
",2024-10-24T15:18:51Z
"A generic approach for reactive stateful mitigation of application
  failures in distributed robotics systems deployed with Kubernetes","Florian Mirus, Frederik Pasch, Nikhil Singhal, Kay-Ulrich Scholl","  Offloading computationally expensive algorithms to the edge or even cloud
offers an attractive option to tackle limitations regarding on-board
computational and energy resources of robotic systems. In cloud-native
applications deployed with the container management system Kubernetes (K8s),
one key problem is ensuring resilience against various types of failures.
However, complex robotic systems interacting with the physical world pose a
very specific set of challenges and requirements that are not yet covered by
failure mitigation approaches from the cloud-native domain. In this paper, we
therefore propose a novel approach for robotic system monitoring and stateful,
reactive failure mitigation for distributed robotic systems deployed using
Kubernetes (K8s) and the Robot Operating System (ROS2). By employing the
generic substrate of Behaviour Trees, our approach can be applied to any
robotic workload and supports arbitrarily complex monitoring and failure
mitigation strategies. We demonstrate the effectiveness and
application-agnosticism of our approach on two example applications, namely
Autonomous Mobile Robot (AMR) navigation and robotic manipulation in a
simulated environment.
",2024-10-24T15:17:09Z
PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models,"Yulian Sun, Li Duan, Yong Li","  Privacy vulnerabilities in LLMs, such as leakage from memorization, have been
constantly identified, and various mitigation proposals have been proposed.
LoRA is usually used in fine-tuning LLMs and a good entry point to insert
privacy-enhancing modules. In this ongoing research, we introduce PSY, a
Posterior Sampling based PrivacY enhancer that can be used in LoRA. We propose
a simple yet effective realization of PSY using posterior sampling, which
effectively prevents privacy leakage from intermediate information and, in
turn, preserves the privacy of data owners. We evaluate LoRA extended with PSY
against state-of-the-art membership inference and data extraction attacks. The
experiments are executed on three different LLM architectures fine-tuned on
three datasets with LoRA. In contrast to the commonly used differential privacy
method, we find that our proposed modification consistently reduces the attack
success rate. Meanwhile, our method has almost no negative impact on model
fine-tuning or final performance. Most importantly, PSY reveals a promising
path toward privacy enhancement with latent space extensions.
",2024-10-24T15:15:42Z
Towards Visual Text Design Transfer Across Languages,"Yejin Choi, Jiwan Chung, Sumin Shim, Giyeong Oh, Youngjae Yu","  Visual text design plays a critical role in conveying themes, emotions, and
atmospheres in multimodal formats such as film posters and album covers.
Translating these visual and textual elements across languages extends the
concept of translation beyond mere text, requiring the adaptation of aesthetic
and stylistic features. To address this, we introduce a novel task of
Multimodal Style Translation (MuST-Bench), a benchmark designed to evaluate the
ability of visual text generation models to perform translation across
different writing systems while preserving design intent. Our initial
experiments on MuST-Bench reveal that existing visual text generation models
struggle with the proposed task due to the inadequacy of textual descriptions
in conveying visual design. In response, we introduce SIGIL, a framework for
multimodal style translation that eliminates the need for style descriptions.
SIGIL enhances image generation models through three innovations: glyph latent
for multilingual settings, pretrained VAEs for stable style guidance, and an
OCR model with reinforcement learning feedback for optimizing readable
character generation. SIGIL outperforms existing baselines by achieving
superior style consistency and legibility while maintaining visual fidelity,
setting itself apart from traditional description-based approaches. We release
MuST-Bench publicly for broader use and exploration
https://huggingface.co/datasets/yejinc/MuST-Bench.
",2024-10-24T15:15:01Z
"Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse
  View Synthesis","Liang Han, Junsheng Zhou, Yu-Shen Liu, Zhizhong Han","  Novel view synthesis from sparse inputs is a vital yet challenging task in 3D
computer vision. Previous methods explore 3D Gaussian Splatting with neural
priors (e.g. depth priors) as an additional supervision, demonstrating
promising quality and efficiency compared to the NeRF based methods. However,
the neural priors from 2D pretrained models are often noisy and blurry, which
struggle to precisely guide the learning of radiance fields. In this paper, We
propose a novel method for synthesizing novel views from sparse views with
Gaussian Splatting that does not require external prior as supervision. Our key
idea lies in exploring the self-supervisions inherent in the binocular stereo
consistency between each pair of binocular images constructed with
disparity-guided image warping. To this end, we additionally introduce a
Gaussian opacity constraint which regularizes the Gaussian locations and avoids
Gaussian redundancy for improving the robustness and efficiency of inferring 3D
Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and
Blender datasets demonstrate that our method significantly outperforms the
state-of-the-art methods.
",2024-10-24T15:10:27Z
"Deterministic $(2/3-\varepsilon)$-Approximation of Matroid Intersection
  using Nearly-Linear Independence-Oracle Queries",Tatsuya Terao,"  In the matroid intersection problem, we are given two matroids $\mathcal{M}_1
= (V, \mathcal{I}_1)$ and $\mathcal{M}_2 = (V, \mathcal{I}_2)$ defined on the
same ground set $V$ of $n$ elements, and the objective is to find a common
independent set $S \in \mathcal{I}_1 \cap \mathcal{I}_2$ of largest possible
cardinality, denoted by $r$. In this paper, we consider a deterministic matroid
intersection algorithm with only a nearly linear number of independence oracle
queries. Our contribution is to present a deterministic
$O(\frac{n}{\varepsilon} + r \log r)$-independence-query
$(2/3-\varepsilon)$-approximation algorithm for any $\varepsilon > 0$. Our idea
is very simple: we apply a recent $\tilde{O}(n
\sqrt{r}/\varepsilon)$-independence-query $(1 - \varepsilon)$-approximation
algorithm of Blikstad [ICALP 2021], but terminate it before completion.
Moreover, we also present a semi-streaming algorithm for $(2/3
-\varepsilon)$-approximation of matroid intersection in $O(1/\varepsilon)$
passes.
",2024-10-24T15:08:38Z
"From Imitation to Introspection: Probing Self-Consciousness in Language
  Models","Sirui Chen, Shu Yu, Shengjie Zhao, Chaochao Lu","  Self-consciousness, the introspection of one's existence and thoughts,
represents a high-level cognitive process. As language models advance at an
unprecedented pace, a critical question arises: Are these models becoming
self-conscious? Drawing upon insights from psychological and neural science,
this work presents a practical definition of self-consciousness for language
models and refines ten core concepts. Our work pioneers an investigation into
self-consciousness in language models by, for the first time, leveraging causal
structural games to establish the functional definitions of the ten core
concepts. Based on our definitions, we conduct a comprehensive four-stage
experiment: quantification (evaluation of ten leading models), representation
(visualization of self-consciousness within the models), manipulation
(modification of the models' representation), and acquisition (fine-tuning the
models on core concepts). Our findings indicate that although models are in the
early stages of developing self-consciousness, there is a discernible
representation of certain concepts within their internal mechanisms. However,
these representations of self-consciousness are hard to manipulate positively
at the current stage, yet they can be acquired through targeted fine-tuning.
Our datasets and code are at https://github.com/OpenCausaLab/SelfConsciousness.
",2024-10-24T15:08:17Z
"TangibleChannel: An Innovative Data Physicalization System for Visual
  Channel Education","Siqi Xie, Yu Liu, Lingyun Yu","  In this paper, we provide an overview of our attempts to harness data
physicalizations as pedagogical tools for enhancing the understanding of visual
channels. We first elaborate the research goals that we have crafted for the
physicalization prototype, shedding light on the key principles that guided our
design choices. Then we detail the materials and datasets we employed for nine
channels on our physicalization prototype. A preliminary pilot study is
followed to validate its effectiveness. In the end, we present our upcoming
research initiatives, including a comparative study for assessing the usability
of the physicalization system. In general, the main purpose of our work is to
stimulate a wider engagement among visualization educators and researchers,
encouraging them to delve into the potentialities of data physicalization as an
innovative addition to contemporary teaching methodologies.
",2024-10-24T14:57:46Z
"Learning Global Object-Centric Representations via Disentangled Slot
  Attention","Tonglin Chen, Yinxuan Huang, Zhimeng Shen, Jinghao Huang, Bin Li, Xiangyang Xue","  Humans can discern scene-independent features of objects across various
environments, allowing them to swiftly identify objects amidst changing factors
such as lighting, perspective, size, and position and imagine the complete
images of the same object in diverse settings. Existing object-centric learning
methods only extract scene-dependent object-centric representations, lacking
the ability to identify the same object across scenes as humans. Moreover, some
existing methods discard the individual object generation capabilities to
handle complex scenes. This paper introduces a novel object-centric learning
method to empower AI systems with human-like capabilities to identify objects
across scenes and generate diverse scenes containing specific objects by
learning a set of global object-centric representations. To learn the global
object-centric representations that encapsulate globally invariant attributes
of objects (i.e., the complete appearance and shape), this paper designs a
Disentangled Slot Attention module to convert the scene features into
scene-dependent attributes (such as scale, position and orientation) and
scene-independent representations (i.e., appearance and shape). Experimental
results substantiate the efficacy of the proposed method, demonstrating
remarkable proficiency in global object-centric representation learning, object
identification, scene generation with specific objects and scene decomposition.
",2024-10-24T14:57:00Z
"Delving into the Reversal Curse: How Far Can Large Language Models
  Generalize?","Zhengkai Lin, Zhihang Fu, Kai Liu, Liang Xie, Binbin Lin, Wenxiao Wang, Deng Cai, Yue Wu, Jieping Ye","  While large language models (LLMs) showcase unprecedented capabilities, they
also exhibit certain inherent limitations when facing seemingly trivial tasks.
A prime example is the recently debated ""reversal curse"", which surfaces when
models, having been trained on the fact ""A is B"", struggle to generalize this
knowledge to infer that ""B is A"". In this paper, we examine the manifestation
of the reversal curse across various tasks and delve into both the
generalization abilities and the problem-solving mechanisms of LLMs. This
investigation leads to a series of significant insights: (1) LLMs are able to
generalize to ""B is A"" when both A and B are presented in the context as in the
case of a multiple-choice question. (2) This generalization ability is highly
correlated to the structure of the fact ""A is B"" in the training documents. For
example, this generalization only applies to biographies structured in ""[Name]
is [Description]"" but not to ""[Description] is [Name]"". (3) We propose and
verify the hypothesis that LLMs possess an inherent bias in fact recalling
during knowledge application, which explains and underscores the importance of
the document structure to successful learning. (4) The negative impact of this
bias on the downstream performance of LLMs can hardly be mitigated through
training alone. Based on these intriguing findings, our work not only presents
a novel perspective for interpreting LLMs' generalization abilities from their
intrinsic working mechanism but also provides new insights for the development
of more effective learning methods for LLMs.
",2024-10-24T14:55:09Z
A Combinatorial Approach to Neural Emergent Communication,Zheyuan Zhang,"  Substantial research on deep learning-based emergent communication uses the
referential game framework, specifically the Lewis signaling game, however we
argue that successful communication in this game typically only need one or two
effective symbols (i.e. message length) because of a sampling pitfall in the
training data. To address this issue, we provide a theoretical analysis and
introduce a combinatorial algorithm SolveMinSym (SMS) to determine the minimum
number of symbols for successful communication min(|M|) in the Lewis signaling
game. We use SMS algorithm to create datasets with different min(|M|) to
empirically show that higher min(|M|) for the training data increases the
number of effective symbols in the emergent language.
",2024-10-24T14:54:09Z
Fast constrained sampling in pre-trained diffusion models,"Alexandros Graikos, Nebojsa Jojic, Dimitris Samaras","  Diffusion models have dominated the field of large, generative image models,
with the prime examples of Stable Diffusion and DALL-E 3 being widely adopted.
These models have been trained to perform text-conditioned generation on vast
numbers of image-caption pairs and as a byproduct, have acquired general
knowledge about natural image statistics. However, when confronted with the
task of constrained sampling, e.g. generating the right half of an image
conditioned on the known left half, applying these models is a delicate and
slow process, with previously proposed algorithms relying on expensive
iterative operations that are usually orders of magnitude slower than
text-based inference. This is counter-intuitive, as image-conditioned
generation should rely less on the difficult-to-learn semantic knowledge that
links captions and imagery, and should instead be achievable by lower-level
correlations among image pixels. In practice, inverse models are trained or
tuned separately for each inverse problem, e.g. by providing parts of images
during training as an additional condition, to allow their application in
realistic settings. However, we argue that this is not necessary and propose an
algorithm for fast-constrained sampling in large pre-trained diffusion models
(Stable Diffusion) that requires no expensive backpropagation operations
through the model and produces results comparable even to the state-of-the-art
\emph{tuned} models. Our method is based on a novel optimization perspective to
sampling under constraints and employs a numerical approximation to the
expensive gradients, previously computed using backpropagation, incurring
significant speed-ups.
",2024-10-24T14:52:38Z
Language-Agnostic Modeling of Source Reliability on Wikipedia,"Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beiró, Pablo Aragón","  Over the last few years, content verification through reliable sources has
become a fundamental need to combat disinformation. Here, we present a
language-agnostic model designed to assess the reliability of sources across
multiple language editions of Wikipedia. Utilizing editorial activity data, the
model evaluates source reliability within different articles of varying
controversiality such as Climate Change, COVID-19, History, Media, and Biology
topics. Crafting features that express domain usage across articles, the model
effectively predicts source reliability, achieving an F1 Macro score of
approximately 0.80 for English and other high-resource languages. For
mid-resource languages, we achieve 0.65 while the performance of low-resource
languages varies; in all cases, the time the domain remains present in the
articles (which we dub as permanence) is one of the most predictive features.
We highlight the challenge of maintaining consistent model performance across
languages of varying resource levels and demonstrate that adapting models from
higher-resource languages can improve performance. This work contributes not
only to Wikipedia's efforts in ensuring content verifiability but in ensuring
reliability across diverse user-generated content in various language
communities.
",2024-10-24T14:52:21Z
"PointPatchRL -- Masked Reconstruction Improves Reinforcement Learning on
  Point Clouds","Balázs Gyenes, Nikolai Franke, Philipp Becker, Gerhard Neumann","  Perceiving the environment via cameras is crucial for Reinforcement Learning
(RL) in robotics. While images are a convenient form of representation, they
often complicate extracting important geometric details, especially with
varying geometries or deformable objects. In contrast, point clouds naturally
represent this geometry and easily integrate color and positional data from
multiple camera views. However, while deep learning on point clouds has seen
many recent successes, RL on point clouds is under-researched, with only the
simplest encoder architecture considered in the literature. We introduce
PointPatchRL (PPRL), a method for RL on point clouds that builds on the common
paradigm of dividing point clouds into overlapping patches, tokenizing them,
and processing the tokens with transformers. PPRL provides significant
improvements compared with other point-cloud processing architectures
previously used for RL. We then complement PPRL with masked reconstruction for
representation learning and show that our method outperforms strong model-free
and model-based baselines on image observations in complex manipulation tasks
containing deformable objects and variations in target object geometry. Videos
and code are available at https://alrhub.github.io/pprl-website
",2024-10-24T14:51:09Z
Arbitrary-arity Tree Automata and QCTL,"François Laroussinie, Nicolas Markey","  We introduce a new class of automata (which we coin EU-automata) running on
infininte trees of arbitrary (finite) arity. We develop and study several
algorithms to perform classical operations (union, intersection, complement,
projection, alternation removal) for those automata, and precisely characterise
their complexities. We also develop algorithms for solving membership and
emptiness for the languages of trees accepted by EU-automata.
  We then use EU-automata to obtain several algorithmic and expressiveness
results for the temporal logic QCTL (which extends CTL with quantification over
atomic propositions) and for MSO. On the one hand, we obtain decision
procedures with optimal complexity for QCTL satisfiability and model checking;
on the other hand, we obtain an algorithm for translating any QCTL formula with
k quantifier alternations to formulas with at most one quantifier alternation,
at the expense of a $(k + 1)$-exponential blow-up in the size of the formulas.
Using the same techniques, we prove that any MSO formula can be translated into
a formula with at most four quantifier alternations (and only two
second-order-quantifier alternations), again with a $(k + 1)$-exponential
blow-up in the size of the formula.
",2024-10-24T14:51:00Z
Distill Visual Chart Reasoning Ability from LLMs to MLLMs,"Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, Xuanjing Huang","  Solving complex chart Q&A tasks requires advanced visual reasoning abilities
in multimodal large language models (MLLMs). Recent studies highlight that
these abilities consist of two main parts: recognizing key information from
visual inputs and conducting reasoning over it. Thus, a promising approach to
enhance MLLMs is to construct relevant training data focusing on the two
aspects. However, collecting and annotating complex charts and questions is
costly and time-consuming, and ensuring the quality of annotated answers
remains a challenge. In this paper, we propose Code-as-Intermediary Translation
(CIT), a cost-effective, efficient and easily scalable data synthesis method
for distilling visual reasoning abilities from LLMs to MLLMs. The code serves
as an intermediary that translates visual chart representations into textual
representations, enabling LLMs to understand cross-modal information.
Specifically, we employ text-based synthesizing techniques to construct
chart-plotting code and produce ReachQA, a dataset containing 3k
reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and
reasoning abilities. Experiments show that when fine-tuned with our data,
models not only perform well on chart-related benchmarks, but also demonstrate
improved multimodal reasoning abilities on general mathematical benchmarks like
MathVista. The code and dataset are publicly available at
https://github.com/hewei2001/ReachQA.
",2024-10-24T14:50:42Z
