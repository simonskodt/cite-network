<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Acs.%2A%26id_list%3D%26start%3D0%26max_results%3D50" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:cs.*&amp;id_list=&amp;start=0&amp;max_results=50</title>
  <id>http://arxiv.org/api/tgey6sasgm1mkL8w2HVV6Y4ARuQ</id>
  <updated>2024-10-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">683485</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">50</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2410.18979v1</id>
    <updated>2024-10-24T17:59:58Z</updated>
    <published>2024-10-24T17:59:58Z</published>
    <title>PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary
  Views</title>
    <summary>  We propose PixelGaussian, an efficient feed-forward framework for learning
generalizable 3D Gaussian reconstruction from arbitrary views. Most existing
methods rely on uniform pixel-wise Gaussian representations, which learn a
fixed number of 3D Gaussians for each view and cannot generalize well to more
input views. Differently, our PixelGaussian dynamically adapts both the
Gaussian distribution and quantity based on geometric complexity, leading to
more efficient representations and significant improvements in reconstruction
quality. Specifically, we introduce a Cascade Gaussian Adapter to adjust
Gaussian distribution according to local geometry complexity identified by a
keypoint scorer. CGA leverages deformable attention in context-aware
hypernetworks to guide Gaussian pruning and splitting, ensuring accurate
representation in complex regions while reducing redundancy. Furthermore, we
design a transformer-based Iterative Gaussian Refiner module that refines
Gaussian representations through direct image-Gaussian interactions. Our
PixelGaussian can effectively reduce Gaussian redundancy as input views
increase. We conduct extensive experiments on the large-scale ACID and
RealEstate10K datasets, where our method achieves state-of-the-art performance
with good generalization to various numbers of views. Code:
https://github.com/Barrybarry-Smith/PixelGaussian.
</summary>
    <author>
      <name>Xin Fei</name>
    </author>
    <author>
      <name>Wenzhao Zheng</name>
    </author>
    <author>
      <name>Yueqi Duan</name>
    </author>
    <author>
      <name>Wei Zhan</name>
    </author>
    <author>
      <name>Masayoshi Tomizuka</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at:
  https://github.com/Barrybarry-Smith/PixelGaussian</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18978v1</id>
    <updated>2024-10-24T17:59:51Z</updated>
    <published>2024-10-24T17:59:51Z</published>
    <title>Framer: Interactive Frame Interpolation</title>
    <summary>  We propose Framer for interactive frame interpolation, which targets
producing smoothly transitioning frames between two images as per user
creativity. Concretely, besides taking the start and end frames as inputs, our
approach supports customizing the transition process by tailoring the
trajectory of some selected keypoints. Such a design enjoys two clear benefits.
First, incorporating human interaction mitigates the issue arising from
numerous possibilities of transforming one image to another, and in turn
enables finer control of local motions. Second, as the most basic form of
interaction, keypoints help establish the correspondence across frames,
enhancing the model to handle challenging cases (e.g., objects on the start and
end frames are of different shapes and styles). It is noteworthy that our
system also offers an "autopilot" mode, where we introduce a module to estimate
the keypoints and refine the trajectory automatically, to simplify the usage in
practice. Extensive experimental results demonstrate the appealing performance
of Framer on various applications, such as image morphing, time-lapse video
generation, cartoon interpolation, etc. The code, the model, and the interface
will be released to facilitate further research.
</summary>
    <author>
      <name>Wen Wang</name>
    </author>
    <author>
      <name>Qiuyu Wang</name>
    </author>
    <author>
      <name>Kecheng Zheng</name>
    </author>
    <author>
      <name>Hao Ouyang</name>
    </author>
    <author>
      <name>Zhekai Chen</name>
    </author>
    <author>
      <name>Biao Gong</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Yujun Shen</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://aim-uofa.github.io/Framer/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18977v1</id>
    <updated>2024-10-24T17:59:45Z</updated>
    <published>2024-10-24T17:59:45Z</published>
    <title>MotionCLR: Motion Generation and Training-free Editing via Understanding
  Attention Mechanisms</title>
    <summary>  This research delves into the problem of interactive editing of human motion
generation. Previous motion diffusion models lack explicit modeling of the
word-level text-motion correspondence and good explainability, hence
restricting their fine-grained editing ability. To address this issue, we
propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR
modeling of attention mechanisms. Technically, MotionCLR models the in-modality
and cross-modality interactions with self-attention and cross-attention,
respectively. More specifically, the self-attention mechanism aims to measure
the sequential similarity between frames and impacts the order of motion
features. By contrast, the cross-attention mechanism works to find the
fine-grained word-sequence correspondence and activate the corresponding
timesteps in the motion sequence. Based on these key properties, we develop a
versatile set of simple yet effective motion editing methods via manipulating
attention maps, such as motion (de-)emphasizing, in-place motion replacement,
and example-based motion generation, etc. For further verification of the
explainability of the attention mechanism, we additionally explore the
potential of action-counting and grounded motion generation ability via
attention maps. Our experimental results show that our method enjoys good
generation and editing ability with good explainability.
</summary>
    <author>
      <name>Ling-Hao Chen</name>
    </author>
    <author>
      <name>Wenxun Dai</name>
    </author>
    <author>
      <name>Xuan Ju</name>
    </author>
    <author>
      <name>Shunlin Lu</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MotionCLR v1 technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18976v1</id>
    <updated>2024-10-24T17:59:38Z</updated>
    <published>2024-10-24T17:59:38Z</published>
    <title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
    <summary>  Recent years have witnessed a significant interest in developing large
multimodal models (LMMs) capable of performing various visual reasoning and
understanding tasks. This has led to the introduction of multiple LMM
benchmarks to evaluate LMMs on different tasks. However, most existing LMM
evaluation benchmarks are predominantly English-centric. In this work, we
develop a comprehensive LMM evaluation benchmark for the Arabic language to
represent a large population of over 400 million speakers. The proposed
benchmark, named CAMEL-Bench, comprises eight diverse domains and 38
sub-domains including, multi-image understanding, complex visual perception,
handwritten document understanding, video understanding, medical imaging, plant
diseases, and remote sensing-based land use understanding to evaluate broad
scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions
that are filtered from a larger pool of samples, where the quality is manually
verified by native speakers to ensure reliable model assessment. We conduct
evaluations of both closed-source, including GPT-4 series, and open-source
LMMs. Our analysis reveals the need for substantial improvement, especially
among the best open-source models, with even the closed-source GPT-4o achieving
an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.
</summary>
    <author>
      <name>Sara Ghaboura</name>
    </author>
    <author>
      <name>Ahmed Heakl</name>
    </author>
    <author>
      <name>Omkar Thawakar</name>
    </author>
    <author>
      <name>Ali Alharthi</name>
    </author>
    <author>
      <name>Ines Riahi</name>
    </author>
    <author>
      <name>Abduljalil Saif</name>
    </author>
    <author>
      <name>Jorma Laaksonen</name>
    </author>
    <author>
      <name>Fahad S. Khan</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Rao M. Anwer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, NAACL</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18975v1</id>
    <updated>2024-10-24T17:59:31Z</updated>
    <published>2024-10-24T17:59:31Z</published>
    <title>Unbounded: A Generative Infinite Game of Character Life Simulation</title>
    <summary>  We introduce the concept of a generative infinite game, a video game that
transcends the traditional boundaries of finite, hard-coded systems by using
generative models. Inspired by James P. Carse's distinction between finite and
infinite games, we leverage recent advances in generative AI to create
Unbounded: a game of character life simulation that is fully encapsulated in
generative models. Specifically, Unbounded draws inspiration from sandbox life
simulations and allows you to interact with your autonomous virtual character
in a virtual world by feeding, playing with and guiding it - with open-ended
mechanics generated by an LLM, some of which can be emergent. In order to
develop Unbounded, we propose technical innovations in both the LLM and visual
generation domains. Specifically, we present: (1) a specialized, distilled
large language model (LLM) that dynamically generates game mechanics,
narratives, and character interactions in real-time, and (2) a new dynamic
regional image prompt Adapter (IP-Adapter) for vision models that ensures
consistent yet flexible visual generation of a character across multiple
environments. We evaluate our system through both qualitative and quantitative
analysis, showing significant improvements in character life simulation, user
instruction following, narrative coherence, and visual consistency for both
characters and the environments compared to traditional related approaches.
</summary>
    <author>
      <name>Jialu Li</name>
    </author>
    <author>
      <name>Yuanzhen Li</name>
    </author>
    <author>
      <name>Neal Wadhwa</name>
    </author>
    <author>
      <name>Yael Pritch</name>
    </author>
    <author>
      <name>David E. Jacobs</name>
    </author>
    <author>
      <name>Michael Rubinstein</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <author>
      <name>Nataniel Ruiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages; Project page: https://generative-infinite-game.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18974v1</id>
    <updated>2024-10-24T17:59:30Z</updated>
    <published>2024-10-24T17:59:30Z</published>
    <title>3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D
  Generation</title>
    <summary>  Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
</summary>
    <author>
      <name>Hansheng Chen</name>
    </author>
    <author>
      <name>Bokui Shen</name>
    </author>
    <author>
      <name>Yulin Liu</name>
    </author>
    <author>
      <name>Ruoxi Shi</name>
    </author>
    <author>
      <name>Linqi Zhou</name>
    </author>
    <author>
      <name>Connor Z. Lin</name>
    </author>
    <author>
      <name>Jiayuan Gu</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Gordon Wetzstein</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://lakonik.github.io/3d-adapter/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18973v1</id>
    <updated>2024-10-24T17:59:23Z</updated>
    <published>2024-10-24T17:59:23Z</published>
    <title>Tuning-free coreset Markov chain Monte Carlo</title>
    <summary>  A Bayesian coreset is a small, weighted subset of a data set that replaces
the full data during inference to reduce computational cost. The
state-of-the-art coreset construction algorithm, Coreset Markov chain Monte
Carlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the
coreset posterior to train the coreset weights via stochastic gradient
optimization. However, the quality of the constructed coreset, and thus the
quality of its posterior approximation, is sensitive to the stochastic
optimization learning rate. In this work, we propose a learning-rate-free
stochastic gradient optimization procedure, Hot-start Distance over Gradient
(Hot DoG), for training coreset weights in Coreset MCMC without user tuning
effort. Empirical results demonstrate that Hot DoG provides higher quality
posterior approximations than other learning-rate-free stochastic gradient
methods, and performs competitively to optimally-tuned ADAM.
</summary>
    <author>
      <name>Naitong Chen</name>
    </author>
    <author>
      <name>Jonathan H. Huggins</name>
    </author>
    <author>
      <name>Trevor Campbell</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18972v1</id>
    <updated>2024-10-24T17:59:21Z</updated>
    <published>2024-10-24T17:59:21Z</published>
    <title>Deep Insights into Cognitive Decline: A Survey of Leveraging
  Non-Intrusive Modalities with Deep Learning Techniques</title>
    <summary>  Cognitive decline is a natural part of aging, often resulting in reduced
cognitive abilities. In some cases, however, this decline is more pronounced,
typically due to disorders such as Alzheimer's disease. Early detection of
anomalous cognitive decline is crucial, as it can facilitate timely
professional intervention. While medical data can help in this detection, it
often involves invasive procedures. An alternative approach is to employ
non-intrusive techniques such as speech or handwriting analysis, which do not
necessarily affect daily activities. This survey reviews the most relevant
methodologies that use deep learning techniques to automate the cognitive
decline estimation task, including audio, text, and visual processing. We
discuss the key features and advantages of each modality and methodology,
including state-of-the-art approaches like Transformer architecture and
foundation models. In addition, we present works that integrate different
modalities to develop multimodal models. We also highlight the most significant
datasets and the quantitative results from studies using these resources. From
this review, several conclusions emerge. In most cases, the textual modality
achieves the best results and is the most relevant for detecting cognitive
decline. Moreover, combining various approaches from individual modalities into
a multimodal model consistently enhances performance across nearly all
scenarios.
</summary>
    <author>
      <name>David Ortiz-Perez</name>
    </author>
    <author>
      <name>Manuel Benavent-Lledo</name>
    </author>
    <author>
      <name>Jose Garcia-Rodriguez</name>
    </author>
    <author>
      <name>David Tomás</name>
    </author>
    <author>
      <name>M. Flores Vizcaya-Moreno</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18970v1</id>
    <updated>2024-10-24T17:59:16Z</updated>
    <published>2024-10-24T17:59:16Z</published>
    <title>ConceptDrift: Uncovering Biases through the Lens of Foundational Models</title>
    <summary>  Datasets and pre-trained models come with intrinsic biases. Most methods rely
on spotting them by analysing misclassified samples, in a semi-automated
human-computer validation. In contrast, we propose ConceptDrift, a method which
analyzes the weights of a linear probe, learned on top a foundational model. We
capitalize on the weight update trajectory, which starts from the embedding of
the textual representation of the class, and proceeds to drift towards
embeddings that disclose hidden biases. Different from prior work, with this
approach we can pin-point unwanted correlations from a dataset, providing more
than just possible explanations for the wrong predictions. We empirically prove
the efficacy of our method, by significantly improving zero-shot performance
with biased-augmented prompting. Our method is not bounded to a single
modality, and we experiment in this work with both image (Waterbirds, CelebA,
Nico++) and text datasets (CivilComments).
</summary>
    <author>
      <name>Cristian Daniel Păduraru</name>
    </author>
    <author>
      <name>Antonio Bărbălau</name>
    </author>
    <author>
      <name>Radu Filipescu</name>
    </author>
    <author>
      <name>Andrei Liviu Nicolicioiu</name>
    </author>
    <author>
      <name>Elena Burceanu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18969v1</id>
    <updated>2024-10-24T17:59:14Z</updated>
    <published>2024-10-24T17:59:14Z</published>
    <title>Self-Improving Autonomous Underwater Manipulation</title>
    <summary>  Underwater robotic manipulation faces significant challenges due to complex
fluid dynamics and unstructured environments, causing most manipulation systems
to rely heavily on human teleoperation. In this paper, we introduce AquaBot, a
fully autonomous manipulation system that combines behavior cloning from human
demonstrations with self-learning optimization to improve beyond human
teleoperation performance. With extensive real-world experiments, we
demonstrate AquaBot's versatility across diverse manipulation tasks, including
object grasping, trash sorting, and rescue retrieval. Our real-world
experiments show that AquaBot's self-optimized policy outperforms a human
operator by 41% in speed. AquaBot represents a promising step towards
autonomous and self-improving underwater manipulation systems. We open-source
both hardware and software implementation details.
</summary>
    <author>
      <name>Ruoshi Liu</name>
    </author>
    <author>
      <name>Huy Ha</name>
    </author>
    <author>
      <name>Mengxue Hou</name>
    </author>
    <author>
      <name>Shuran Song</name>
    </author>
    <author>
      <name>Carl Vondrick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://aquabot.cs.columbia.edu/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18967v1</id>
    <updated>2024-10-24T17:58:31Z</updated>
    <published>2024-10-24T17:58:31Z</published>
    <title>Ferret-UI 2: Mastering Universal User Interface Understanding Across
  Platforms</title>
    <summary>  Building a generalist model for user interface (UI) understanding is
challenging due to various foundational issues, such as platform diversity,
resolution variation, and data limitation. In this paper, we introduce
Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI
understanding across a wide range of platforms, including iPhone, Android,
iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI
2 introduces three key innovations: support for multiple platform types,
high-resolution perception through adaptive scaling, and advanced task training
data generation powered by GPT-4o with set-of-mark visual prompting. These
advancements enable Ferret-UI 2 to perform complex, user-centered interactions,
making it highly versatile and adaptable for the expanding diversity of
platform ecosystems. Extensive empirical experiments on referring, grounding,
user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE
next-action prediction dataset, and GUI-World multi-platform benchmark
demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also
shows strong cross-platform transfer capabilities.
</summary>
    <author>
      <name>Zhangheng Li</name>
    </author>
    <author>
      <name>Keen You</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Di Feng</name>
    </author>
    <author>
      <name>Harsh Agrawal</name>
    </author>
    <author>
      <name>Xiujun Li</name>
    </author>
    <author>
      <name>Mohana Prasad Sathya Moorthy</name>
    </author>
    <author>
      <name>Jeff Nichols</name>
    </author>
    <author>
      <name>Yinfei Yang</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18966v1</id>
    <updated>2024-10-24T17:58:22Z</updated>
    <published>2024-10-24T17:58:22Z</published>
    <title>Does Data Contamination Detection Work (Well) for LLMs? A Survey and
  Evaluation on Detection Assumptions</title>
    <summary>  Large language models (LLMs) have demonstrated great performance across
various benchmarks, showing potential as general-purpose task solvers. However,
as LLMs are typically trained on vast amounts of data, a significant concern in
their evaluation is data contamination, where overlap between training data and
evaluation datasets inflates performance assessments. While multiple approaches
have been developed to identify data contamination, these approaches rely on
specific assumptions that may not hold universally across different settings.
To bridge this gap, we systematically review 47 papers on data contamination
detection, categorize the underlying assumptions, and assess whether they have
been rigorously validated. We identify and analyze eight categories of
assumptions and test three of them as case studies. Our analysis reveals that
when classifying instances used for pretraining LLMs, detection approaches
based on these three assumptions perform close to random guessing, suggesting
that current LLMs learn data distributions rather than memorizing individual
instances. Overall, this work underscores the importance of approaches clearly
stating their underlying assumptions and testing their validity across various
scenarios.
</summary>
    <author>
      <name>Yujuan Fu</name>
    </author>
    <author>
      <name>Ozlem Uzuner</name>
    </author>
    <author>
      <name>Meliha Yetisgen</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 tables and 1 figures in the main text. This is a preprint, under
  review</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18965v1</id>
    <updated>2024-10-24T17:58:21Z</updated>
    <published>2024-10-24T17:58:21Z</published>
    <title>On the Crucial Role of Initialization for Matrix Factorization</title>
    <summary>  This work revisits the classical low-rank matrix factorization problem and
unveils the critical role of initialization in shaping convergence rates for
such nonconvex and nonsmooth optimization. We introduce Nystrom initialization,
which significantly improves the global convergence of Scaled Gradient Descent
(ScaledGD) in both symmetric and asymmetric matrix factorization tasks.
Specifically, we prove that ScaledGD with Nystrom initialization achieves
quadratic convergence in cases where only linear rates were previously known.
Furthermore, we extend this initialization to low-rank adapters (LoRA) commonly
used for finetuning foundation models. Our approach, NoRA, i.e., LoRA with
Nystrom initialization, demonstrates superior performance across various
downstream tasks and model scales, from 1B to 7B parameters, in large language
and diffusion models.
</summary>
    <author>
      <name>Bingcong Li</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Aryan Mokhtari</name>
    </author>
    <author>
      <name>Niao He</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18964v1</id>
    <updated>2024-10-24T17:58:11Z</updated>
    <published>2024-10-24T17:58:11Z</published>
    <title>Learning to Look: Seeking Information for Decision Making via Policy
  Factorization</title>
    <summary>  Many robot manipulation tasks require active or interactive exploration
behavior in order to be performed successfully. Such tasks are ubiquitous in
embodied domains, where agents must actively search for the information
necessary for each stage of a task, e.g., moving the head of the robot to find
information relevant to manipulation, or in multi-robot domains, where one
scout robot may search for the information that another robot needs to make
informed decisions. We identify these tasks with a new type of problem,
factorized Contextual Markov Decision Processes, and propose DISaM, a
dual-policy solution composed of an information-seeking policy that explores
the environment to find the relevant contextual information and an
information-receiving policy that exploits the context to achieve the
manipulation goal. This factorization allows us to train both policies
separately, using the information-receiving one to provide reward to train the
information-seeking policy. At test time, the dual agent balances exploration
and exploitation based on the uncertainty the manipulation policy has on what
the next best action is. We demonstrate the capabilities of our dual policy
solution in five manipulation tasks that require information-seeking behaviors,
both in simulation and in the real-world, where DISaM significantly outperforms
existing methods. More information at
https://robin-lab.cs.utexas.edu/learning2look/.
</summary>
    <author>
      <name>Shivin Dass</name>
    </author>
    <author>
      <name>Jiaheng Hu</name>
    </author>
    <author>
      <name>Ben Abbatematteo</name>
    </author>
    <author>
      <name>Peter Stone</name>
    </author>
    <author>
      <name>Roberto Martín-Martín</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website: https://robin-lab.cs.utexas.edu/learning2look/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18963v1</id>
    <updated>2024-10-24T17:58:08Z</updated>
    <published>2024-10-24T17:58:08Z</published>
    <title>OSCAR: Operating System Control via State-Aware Reasoning and
  Re-Planning</title>
    <summary>  Large language models (LLMs) and large multimodal models (LMMs) have shown
great potential in automating complex tasks like web browsing and gaming.
However, their ability to generalize across diverse applications remains
limited, hindering broader utility. To address this challenge, we present
OSCAR: Operating System Control via state-Aware reasoning and Re-planning.
OSCAR is a generalist agent designed to autonomously navigate and interact with
various desktop and mobile applications through standardized controls, such as
mouse and keyboard inputs, while processing screen images to fulfill user
commands. OSCAR translates human instructions into executable Python code,
enabling precise control over graphical user interfaces (GUIs). To enhance
stability and adaptability, OSCAR operates as a state machine, equipped with
error-handling mechanisms and dynamic task re-planning, allowing it to
efficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's
effectiveness through extensive experiments on diverse benchmarks across
desktop and mobile platforms, where it transforms complex workflows into simple
natural language commands, significantly boosting user productivity. Our code
will be open-source upon publication.
</summary>
    <author>
      <name>Xiaoqiang Wang</name>
    </author>
    <author>
      <name>Bang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18962v1</id>
    <updated>2024-10-24T17:58:05Z</updated>
    <published>2024-10-24T17:58:05Z</published>
    <title>Where Am I and What Will I See: An Auto-Regressive Model for Spatial
  Localization and View Prediction</title>
    <summary>  Spatial intelligence is the ability of a machine to perceive, reason, and act
in three dimensions within space and time. Recent advancements in large-scale
auto-regressive models have demonstrated remarkable capabilities across various
reasoning tasks. However, these models often struggle with fundamental aspects
of spatial reasoning, particularly in answering questions like "Where am I?"
and "What will I see?". While some attempts have been done, existing approaches
typically treat them as separate tasks, failing to capture their interconnected
nature. In this paper, we present Generative Spatial Transformer (GST), a novel
auto-regressive framework that jointly addresses spatial localization and view
prediction. Our model simultaneously estimates the camera pose from a single
image and predicts the view from a new camera pose, effectively bridging the
gap between spatial awareness and visual prediction. The proposed innovative
camera tokenization method enables the model to learn the joint distribution of
2D projections and their corresponding spatial perspectives in an
auto-regressive manner. This unified training paradigm demonstrates that joint
optimization of pose estimation and novel view synthesis leads to improved
performance in both tasks, for the first time, highlighting the inherent
relationship between spatial awareness and visual prediction.
</summary>
    <author>
      <name>Junyi Chen</name>
    </author>
    <author>
      <name>Di Huang</name>
    </author>
    <author>
      <name>Weicai Ye</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Tong He</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18959v1</id>
    <updated>2024-10-24T17:56:08Z</updated>
    <published>2024-10-24T17:56:08Z</published>
    <title>Context is Key: A Benchmark for Forecasting with Essential Textual
  Information</title>
    <summary>  Forecasting is a critical task in decision making across various domains.
While numerical data provides a foundation, it often lacks crucial context
necessary for accurate predictions. Human forecasters frequently rely on
additional information, such as background knowledge or constraints, which can
be efficiently communicated through natural language. However, the ability of
existing forecasting models to effectively integrate this textual information
remains an open question. To address this, we introduce "Context is Key" (CiK),
a time series forecasting benchmark that pairs numerical data with diverse
types of carefully crafted textual context, requiring models to integrate both
modalities. We evaluate a range of approaches, including statistical models,
time series foundation models, and LLM-based forecasters, and propose a simple
yet effective LLM prompting method that outperforms all other tested methods on
our benchmark. Our experiments highlight the importance of incorporating
contextual information, demonstrate surprising performance when using LLM-based
forecasting models, and also reveal some of their critical shortcomings. By
presenting this benchmark, we aim to advance multimodal forecasting, promoting
models that are both accurate and accessible to decision-makers with varied
technical expertise. The benchmark can be visualized at
https://servicenow.github.io/context-is-key-forecasting/v0/ .
</summary>
    <author>
      <name>Andrew Robert Williams</name>
    </author>
    <author>
      <name>Arjun Ashok</name>
    </author>
    <author>
      <name>Étienne Marcotte</name>
    </author>
    <author>
      <name>Valentina Zantedeschi</name>
    </author>
    <author>
      <name>Jithendaraa Subramanian</name>
    </author>
    <author>
      <name>Roland Riachi</name>
    </author>
    <author>
      <name>James Requeima</name>
    </author>
    <author>
      <name>Alexandre Lacoste</name>
    </author>
    <author>
      <name>Irina Rish</name>
    </author>
    <author>
      <name>Nicolas Chapados</name>
    </author>
    <author>
      <name>Alexandre Drouin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint; under review. First two authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18958v1</id>
    <updated>2024-10-24T17:55:52Z</updated>
    <published>2024-10-24T17:55:52Z</published>
    <title>Stable Consistency Tuning: Understanding and Improving Consistency
  Models</title>
    <summary>  Diffusion models achieve superior generation quality but suffer from slow
generation speed due to the iterative nature of denoising. In contrast,
consistency models, a new generative family, achieve competitive performance
with significantly faster sampling. These models are trained either through
consistency distillation, which leverages pretrained diffusion models, or
consistency training/tuning directly from raw data. In this work, we propose a
novel framework for understanding consistency models by modeling the denoising
process of the diffusion model as a Markov Decision Process (MDP) and framing
consistency model training as the value estimation through Temporal
Difference~(TD) Learning. More importantly, this framework allows us to analyze
the limitations of current consistency training/tuning strategies. Built upon
Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),
which incorporates variance-reduced learning using the score identity. SCT
leads to significant performance improvements on benchmarks such as CIFAR-10
and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID
1.55, a new SoTA for consistency models.
</summary>
    <author>
      <name>Fu-Yun Wang</name>
    </author>
    <author>
      <name>Zhengyang Geng</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at
  https://github.com/G-U-N/Stable-Consistency-Tuning</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18957v1</id>
    <updated>2024-10-24T17:55:03Z</updated>
    <published>2024-10-24T17:55:03Z</published>
    <title>Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in
  Low-Resource Code</title>
    <summary>  Large Language Models (LLMs) demonstrate strong proficiency in generating
code for high-resource programming languages (HRPLs) like Python but struggle
significantly with low-resource programming languages (LRPLs) such as Racket or
D. This performance gap deepens the digital divide, preventing developers using
LRPLs from benefiting equally from LLM advancements and reinforcing disparities
in innovation within underrepresented programming communities. While generating
additional training data for LRPLs is promising, it faces two key challenges:
manual annotation is labor-intensive and costly, and LLM-generated LRPL code is
often of subpar quality. The underlying cause of this issue is the gap between
natural language to programming language gap (NL-PL Gap), which is especially
pronounced in LRPLs due to limited aligned data. In this work, we introduce a
novel approach called Bridge-Coder, which leverages LLMs' intrinsic
capabilities to enhance the performance on LRPLs. Our method consists of two
key stages. Bridge Generation, where we create high-quality dataset by
utilizing LLMs' general knowledge understanding, proficiency in HRPLs, and
in-context learning abilities. Then, we apply the Bridged Alignment, which
progressively improves the alignment between NL instructions and LRPLs.
Experimental results across multiple LRPLs show that Bridge-Coder significantly
enhances model performance, demonstrating the effectiveness and generalization
of our approach. Furthermore, we offer a detailed analysis of the key
components of our method, providing valuable insights for future work aimed at
addressing the challenges associated with LRPLs.
</summary>
    <author>
      <name>Jipeng Zhang</name>
    </author>
    <author>
      <name>Jianshu Zhang</name>
    </author>
    <author>
      <name>Yuanzhe Li</name>
    </author>
    <author>
      <name>Renjie Pi</name>
    </author>
    <author>
      <name>Rui Pan</name>
    </author>
    <author>
      <name>Runtao Liu</name>
    </author>
    <author>
      <name>Ziqiang Zheng</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18956v1</id>
    <updated>2024-10-24T17:54:42Z</updated>
    <published>2024-10-24T17:54:42Z</published>
    <title>Large Spatial Model: End-to-end Unposed Images to Semantic 3D</title>
    <summary>  Reconstructing and understanding 3D structures from a limited number of
images is a well-established problem in computer vision. Traditional methods
usually break this task into multiple subtasks, each requiring complex
transformations between different data representations. For instance, dense
reconstruction through Structure-from-Motion (SfM) involves converting images
into key points, optimizing camera parameters, and estimating structures.
Afterward, accurate sparse reconstructions are required for further dense
modeling, which is subsequently fed into task-specific neural networks. This
multi-step process results in considerable processing time and increased
engineering complexity.
  In this work, we present the Large Spatial Model (LSM), which processes
unposed RGB images directly into semantic radiance fields. LSM simultaneously
estimates geometry, appearance, and semantics in a single feed-forward
operation, and it can generate versatile label maps by interacting with
language at novel viewpoints. Leveraging a Transformer-based architecture, LSM
integrates global geometry through pixel-aligned point maps. To enhance spatial
attribute regression, we incorporate local context aggregation with multi-scale
fusion, improving the accuracy of fine local details. To tackle the scarcity of
labeled 3D semantic data and enable natural language-driven scene manipulation,
we incorporate a pre-trained 2D language-based segmentation model into a
3D-consistent semantic feature field. An efficient decoder then parameterizes a
set of semantic anisotropic Gaussians, facilitating supervised end-to-end
learning. Extensive experiments across various tasks show that LSM unifies
multiple 3D vision tasks directly from unposed images, achieving real-time
semantic 3D reconstruction for the first time.
</summary>
    <author>
      <name>Zhiwen Fan</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Wenyan Cong</name>
    </author>
    <author>
      <name>Peihao Wang</name>
    </author>
    <author>
      <name>Renjie Li</name>
    </author>
    <author>
      <name>Kairun Wen</name>
    </author>
    <author>
      <name>Shijie Zhou</name>
    </author>
    <author>
      <name>Achuta Kadambi</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <author>
      <name>Danfei Xu</name>
    </author>
    <author>
      <name>Boris Ivanovic</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website: https://largespatialmodel.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18955v1</id>
    <updated>2024-10-24T17:53:53Z</updated>
    <published>2024-10-24T17:53:53Z</published>
    <title>BioMistral-NLU: Towards More Generalizable Medical Language
  Understanding through Instruction Tuning</title>
    <summary>  Large language models (LLMs) such as ChatGPT are fine-tuned on large and
diverse instruction-following corpora, and can generalize to new tasks.
However, those instruction-tuned LLMs often perform poorly in specialized
medical natural language understanding (NLU) tasks that require domain
knowledge, granular text comprehension, and structured data extraction. To
bridge the gap, we: (1) propose a unified prompting format for 7 important NLU
tasks, % through span extraction and multi-choice question-answering (QA), (2)
curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing
open-source medical NLU corpora, and (3) develop BioMistral-NLU, a
generalizable medical NLU model, through fine-tuning BioMistral on
MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6
important NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical
Language Understanding Evaluation (BLUE) and Biomedical Language Understanding
and Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU
outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT
and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step
over diverse NLU tasks enhance LLMs' generalizability across diverse medical
NLU tasks. Our ablation experiments show that instruction-tuning on a wider
variety of tasks, even when the total number of training instances remains
constant, enhances downstream zero-shot generalization.
</summary>
    <author>
      <name>Yujuan Velvin Fu</name>
    </author>
    <author>
      <name>Giridhar Kaushik Ramachandran</name>
    </author>
    <author>
      <name>Namu Park</name>
    </author>
    <author>
      <name>Kevin Lybarger</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Ozlem Uzuner</name>
    </author>
    <author>
      <name>Meliha Yetisgen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures an 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18954v1</id>
    <updated>2024-10-24T17:53:33Z</updated>
    <published>2024-10-24T17:53:33Z</published>
    <title>Learning Structured Compressed Sensing with Automatic Resource
  Allocation</title>
    <summary>  Multidimensional data acquisition often requires extensive time and poses
significant challenges for hardware and software regarding data storage and
processing. Rather than designing a single compression matrix as in
conventional compressed sensing, structured compressed sensing yields
dimension-specific compression matrices, reducing the number of optimizable
parameters. Recent advances in machine learning (ML) have enabled task-based
supervised learning of subsampling matrices, albeit at the expense of complex
downstream models. Additionally, the sampling resource allocation across
dimensions is often determined in advance through heuristics. To address these
challenges, we introduce Structured COmpressed Sensing with Automatic Resource
Allocation (SCOSARA) with an information theory-based unsupervised learning
strategy. SCOSARA adaptively distributes samples across sampling dimensions
while maximizing Fisher information content. Using ultrasound localization as a
case study, we compare SCOSARA to state-of-the-art ML-based and greedy search
algorithms. Simulation results demonstrate that SCOSARA can produce
high-quality subsampling matrices that achieve lower Cram\'er-Rao Bound values
than the baselines. In addition, SCOSARA outperforms other ML-based algorithms
in terms of the number of trainable parameters, computational complexity, and
memory requirements while automatically choosing the number of samples per
axis.
</summary>
    <author>
      <name>Han Wang</name>
    </author>
    <author>
      <name>Eduardo Pérez</name>
    </author>
    <author>
      <name>Iris A. M. Huijben</name>
    </author>
    <author>
      <name>Hans van Gorp</name>
    </author>
    <author>
      <name>Ruud van Sloun</name>
    </author>
    <author>
      <name>Florian Römer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Unsupervised Learning, Information Theory, Compressed Sensing,
  Subsampling</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18953v1</id>
    <updated>2024-10-24T17:53:02Z</updated>
    <published>2024-10-24T17:53:02Z</published>
    <title>The Learning Stabilizers with Noise problem</title>
    <summary>  Random classical codes have good error correcting properties, and yet they
are notoriously hard to decode in practice. Despite many decades of extensive
study, the fastest known algorithms still run in exponential time. The Learning
Parity with Noise (LPN) problem, which can be seen as the task of decoding a
random linear code in the presence of noise, has thus emerged as a prominent
hardness assumption with numerous applications in both cryptography and
learning theory.
  Is there a natural quantum analog of the LPN problem? In this work, we
introduce the Learning Stabilizers with Noise (LSN) problem, the task of
decoding a random stabilizer code in the presence of local depolarizing noise.
We give both polynomial-time and exponential-time quantum algorithms for
solving LSN in various depolarizing noise regimes, ranging from extremely low
noise, to low constant noise rates, and even higher noise rates up to a
threshold. Next, we provide concrete evidence that LSN is hard. First, we show
that LSN includes LPN as a special case, which suggests that it is at least as
hard as its classical counterpart. Second, we prove a worst-case to
average-case reduction for variants of LSN. We then ask: what is the
computational complexity of solving LSN? Because the task features quantum
inputs, its complexity cannot be characterized by traditional complexity
classes. Instead, we show that the LSN problem lies in a recently introduced
(distributional and oracle) unitary synthesis class. Finally, we identify
several applications of our LSN assumption, ranging from the construction of
quantum bit commitment schemes to the computational limitations of learning
from quantum data.
</summary>
    <author>
      <name>Alexander Poremba</name>
    </author>
    <author>
      <name>Yihui Quek</name>
    </author>
    <author>
      <name>Peter Shor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">61 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18952v1</id>
    <updated>2024-10-24T17:52:31Z</updated>
    <published>2024-10-24T17:52:31Z</published>
    <title>Dynamic Vocabulary Pruning in Early-Exit LLMs</title>
    <summary>  Increasing the size of large language models (LLMs) has been shown to lead to
better performance. However, this comes at the cost of slower and more
expensive inference. Early-exiting is a promising approach for improving the
efficiency of LLM inference by enabling next token prediction at intermediate
layers. Yet, the large vocabulary size in modern LLMs makes the confidence
estimation required for exit decisions computationally expensive, diminishing
the efficiency gains. To address this, we propose dynamically pruning the
vocabulary at test time for each token. Specifically, the vocabulary is pruned
at one of the initial layers, and the smaller vocabulary is then used
throughout the rest of the forward pass. Our experiments demonstrate that such
post-hoc dynamic vocabulary pruning improves the efficiency of confidence
estimation in early-exit LLMs while maintaining competitive performance.
</summary>
    <author>
      <name>Jort Vincenti</name>
    </author>
    <author>
      <name>Karim Abdel Sadek</name>
    </author>
    <author>
      <name>Joan Velja</name>
    </author>
    <author>
      <name>Matteo Nulli</name>
    </author>
    <author>
      <name>Metod Jazbec</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18950v1</id>
    <updated>2024-10-24T17:50:08Z</updated>
    <published>2024-10-24T17:50:08Z</published>
    <title>Adjusted Overfitting Regression</title>
    <summary>  In this paper, I will introduce a new form of regression, that can adjust
overfitting and underfitting through, "distance-based regression." Overfitting
often results in finding false patterns causing inaccurate results, so by
having a new approach that minimizes overfitting, more accurate predictions can
be derived. Then I will proceed with a test of my regression form and show
additional ways to optimize the regression. Finally, I will apply my new
technique to a specific data set to demonstrate its practical value.
</summary>
    <author>
      <name>Dylan Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18944v1</id>
    <updated>2024-10-24T17:35:12Z</updated>
    <published>2024-10-24T17:35:12Z</published>
    <title>Path Guiding for Monte Carlo PDE Solvers</title>
    <summary>  In recent years, Monte Carlo PDE solvers have garnered increasing attention
in computer graphics, demonstrating value across a wide range of applications.
Despite offering clear advantages over traditional methods-such as avoiding
discretization and enabling local evaluations-Monte Carlo PDE solvers face
challenges due to their stochastic nature, including high variance and slow
convergence rates. To mitigate the variance issue, we draw inspiration from
Monte Carlo path tracing and apply the path guiding technique to the Walk on
Stars estimator. Specifically, we examine the target sampling distribution at
each step of the Walk on Stars estimator, parameterize it, and introduce neural
implicit representations to model the spatially-varying guiding distribution.
This path guiding approach is implemented in a wavefront-style PDE solver, and
experimental results demonstrate that it effectively reduces variance in Monte
Carlo PDE solvers.
</summary>
    <author>
      <name>Tianyu Huang</name>
    </author>
    <author>
      <name>Jingwang Ling</name>
    </author>
    <author>
      <name>Shuang Zhao</name>
    </author>
    <author>
      <name>Feng Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18938v1</id>
    <updated>2024-10-24T17:24:34Z</updated>
    <published>2024-10-24T17:24:34Z</published>
    <title>A Random Matrix Theory Perspective on the Spectrum of Learned Features
  and Asymptotic Generalization Capabilities</title>
    <summary>  A key property of neural networks is their capacity of adapting to data
during training. Yet, our current mathematical understanding of feature
learning and its relationship to generalization remain limited. In this work,
we provide a random matrix analysis of how fully-connected two-layer neural
networks adapt to the target function after a single, but aggressive, gradient
descent step. We rigorously establish the equivalence between the updated
features and an isotropic spiked random feature model, in the limit of large
batch size. For the latter model, we derive a deterministic equivalent
description of the feature empirical covariance matrix in terms of certain
low-dimensional operators. This allows us to sharply characterize the impact of
training in the asymptotic feature spectrum, and in particular, provides a
theoretical grounding for how the tails of the feature spectrum modify with
training. The deterministic equivalent further yields the exact asymptotic
generalization error, shedding light on the mechanisms behind its improvement
in the presence of feature learning. Our result goes beyond standard random
matrix ensembles, and therefore we believe it is of independent technical
interest. Different from previous work, our result holds in the challenging
maximal learning rate regime, is fully rigorous and allows for finitely
supported second layer initialization, which turns out to be crucial for
studying the functional expressivity of the learned features. This provides a
sharp description of the impact of feature learning in the generalization of
two-layer neural networks, beyond the random features and lazy training
regimes.
</summary>
    <author>
      <name>Yatin Dandi</name>
    </author>
    <author>
      <name>Luca Pesce</name>
    </author>
    <author>
      <name>Hugo Cui</name>
    </author>
    <author>
      <name>Florent Krzakala</name>
    </author>
    <author>
      <name>Yue M. Lu</name>
    </author>
    <author>
      <name>Bruno Loureiro</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18936v1</id>
    <updated>2024-10-24T17:22:24Z</updated>
    <published>2024-10-24T17:22:24Z</published>
    <title>Matching Composition and Efficient Weight Reduction in Dynamic Matching</title>
    <summary>  We consider the foundational problem of maintaining a
$(1-\varepsilon)$-approximate maximum weight matching (MWM) in an $n$-node
dynamic graph undergoing edge insertions and deletions. We provide a general
reduction that reduces the problem on graphs with a weight range of
$\mathrm{poly}(n)$ to $\mathrm{poly}(1/\varepsilon)$ at the cost of just an
additive $\mathrm{poly}(1/\varepsilon)$ in update time. This improves upon the
prior reduction of Gupta-Peng (FOCS 2013) which reduces the problem to a weight
range of $\varepsilon^{-O(1/\varepsilon)}$ with a multiplicative cost of
$O(\log n)$.
  When combined with a reduction of Bernstein-Dudeja-Langley (STOC 2021) this
yields a reduction from dynamic $(1-\varepsilon)$-approximate MWM in bipartite
graphs with a weight range of $\mathrm{poly}(n)$ to dynamic
$(1-\varepsilon)$-approximate maximum cardinality matching in bipartite graphs
at the cost of a multiplicative $\mathrm{poly}(1/\varepsilon)$ in update time,
thereby resolving an open problem in [GP'13; BDL'21]. Additionally, we show
that our approach is amenable to MWM problems in streaming, shared-memory
work-depth, and massively parallel computation models. We also apply our
techniques to obtain an efficient dynamic algorithm for rounding weighted
fractional matchings in general graphs. Underlying our framework is a new
structural result about MWM that we call the "matching composition lemma" and
new dynamic matching subroutines that may be of independent interest.
</summary>
    <author>
      <name>Aaron Bernstein</name>
    </author>
    <author>
      <name>Jiale Chen</name>
    </author>
    <author>
      <name>Aditi Dudeja</name>
    </author>
    <author>
      <name>Zachary Langley</name>
    </author>
    <author>
      <name>Aaron Sidford</name>
    </author>
    <author>
      <name>Ta-Wei Tu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18935v1</id>
    <updated>2024-10-24T17:21:43Z</updated>
    <published>2024-10-24T17:21:43Z</published>
    <title>Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent
  Role-Play</title>
    <summary>  Complex news events, such as natural disasters and socio-political conflicts,
require swift responses from the government and society. Relying on historical
events to project the future is insufficient as such events are sparse and do
not cover all possible conditions and nuanced situations. Simulation of these
complex events can help better prepare and reduce the negative impact. We
develop a controllable complex news event simulator guided by both the event
schema representing domain knowledge about the scenario and user-provided
assumptions representing case-specific conditions. As event dynamics depend on
the fine-grained social and cultural context, we further introduce a
geo-diverse commonsense and cultural norm-aware knowledge enhancement
component. To enhance the coherence of the simulation, apart from the global
timeline of events, we take an agent-based approach to simulate the individual
character states, plans, and actions. By incorporating the schema and cultural
norms, our generated simulations achieve much higher coherence and
appropriateness and are received favorably by participants from a humanitarian
assistance organization.
</summary>
    <author>
      <name>Sha Li</name>
    </author>
    <author>
      <name>Revanth Gangi Reddy</name>
    </author>
    <author>
      <name>Khanh Duy Nguyen</name>
    </author>
    <author>
      <name>Qingyun Wang</name>
    </author>
    <author>
      <name>May Fung</name>
    </author>
    <author>
      <name>Chi Han</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <author>
      <name>Kartik Natarajan</name>
    </author>
    <author>
      <name>Clare R. Voss</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as EMNLP 2024 Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18932v1</id>
    <updated>2024-10-24T17:19:53Z</updated>
    <published>2024-10-24T17:19:53Z</published>
    <title>ANAVI: Audio Noise Awareness using Visuals of Indoor environments for
  NAVIgation</title>
    <summary>  We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for
quieter robot path planning. While humans are naturally aware of the noise they
make and its impact on those around them, robots currently lack this awareness.
A key challenge in achieving audio awareness for robots is estimating how loud
will the robot's actions be at a listener's location? Since sound depends upon
the geometry and material composition of rooms, we train the robot to passively
perceive loudness using visual observations of indoor environments. To this
end, we generate data on how loud an 'impulse' sounds at different listener
locations in simulated homes, and train our Acoustic Noise Predictor (ANP).
Next, we collect acoustic profiles corresponding to different actions for
navigation. Unifying ANP with action acoustics, we demonstrate experiments with
wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these
robots adhere to the noise constraints of the environment. See code and data at
https://anavi-corl24.github.io/
</summary>
    <author>
      <name>Vidhi Jain</name>
    </author>
    <author>
      <name>Rishi Veerapaneni</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8th Conference on Robot Learning (CoRL) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18931v1</id>
    <updated>2024-10-24T17:18:01Z</updated>
    <published>2024-10-24T17:18:01Z</published>
    <title>Sort-free Gaussian Splatting via Weighted Sum Rendering</title>
    <summary>  Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant
advancement in 3D scene reconstruction, attracting considerable attention due
to its ability to recover high-fidelity details while maintaining low
complexity. Despite the promising results achieved by 3DGS, its rendering
performance is constrained by its dependence on costly non-commutative
alpha-blending operations. These operations mandate complex view dependent
sorting operations that introduce computational overhead, especially on the
resource-constrained platforms such as mobile phones. In this paper, we propose
Weighted Sum Rendering, which approximates alpha blending with weighted sums,
thereby removing the need for sorting. This simplifies implementation, delivers
superior performance, and eliminates the "popping" artifacts caused by sorting.
Experimental results show that optimizing a generalized Gaussian splatting
formulation to the new differentiable rendering yields competitive image
quality. The method was implemented and tested in a mobile device GPU,
achieving on average $1.23\times$ faster rendering.
</summary>
    <author>
      <name>Qiqi Hou</name>
    </author>
    <author>
      <name>Randall Rauwendaal</name>
    </author>
    <author>
      <name>Zifeng Li</name>
    </author>
    <author>
      <name>Hoang Le</name>
    </author>
    <author>
      <name>Farzad Farhadzadeh</name>
    </author>
    <author>
      <name>Fatih Porikli</name>
    </author>
    <author>
      <name>Alexei Bourd</name>
    </author>
    <author>
      <name>Amir Said</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18929v1</id>
    <updated>2024-10-24T17:17:11Z</updated>
    <published>2024-10-24T17:17:11Z</published>
    <title>AutoStep: Locally adaptive involutive MCMC</title>
    <summary>  Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a
deterministic involutive proposal with a step size parameter. Selecting an
appropriate step size is often a challenging task in practice; and for complex
multiscale targets, there may not be one choice of step size that works well
globally. In this work, we address this problem with a novel class of
involutive MCMC methods -- AutoStep MCMC -- that selects an appropriate step
size at each iteration adapted to the local geometry of the target
distribution. We prove that AutoStep MCMC is $\pi$-invariant and has other
desirable properties under mild assumptions on the target distribution $\pi$
and involutive proposal. Empirical results examine the effect of various step
size selection design choices, and show that AutoStep MCMC is competitive with
state-of-the-art methods in terms of effective sample size per unit cost on a
range of challenging target distributions.
</summary>
    <author>
      <name>Tiange Liu</name>
    </author>
    <author>
      <name>Nikola Surjanovic</name>
    </author>
    <author>
      <name>Miguel Biron-Lattes</name>
    </author>
    <author>
      <name>Alexandre Bouchard-Côté</name>
    </author>
    <author>
      <name>Trevor Campbell</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18928v1</id>
    <updated>2024-10-24T17:16:19Z</updated>
    <published>2024-10-24T17:16:19Z</published>
    <title>Learning $k$-body Hamiltonians via compressed sensing</title>
    <summary>  We study the problem of learning a $k$-body Hamiltonian with $M$ unknown
Pauli terms that are not necessarily geometrically local. We propose a protocol
that learns the Hamiltonian to precision $\epsilon$ with total evolution time
${\mathcal{O}}(M^{1/2+1/p}/\epsilon)$ up to logarithmic factors, where the
error is quantified by the $\ell^p$-distance between Pauli coefficients. Our
learning protocol uses only single-qubit control operations and a GHZ state
initial state, is non-adaptive, is robust against SPAM errors, and performs
well even if $M$ and $k$ are not precisely known in advance or if the
Hamiltonian is not exactly $M$-sparse. Methods from the classical theory of
compressed sensing are used for efficiently identifying the $M$ terms in the
Hamiltonian from among all possible $k$-body Pauli operators. We also provide a
lower bound on the total evolution time needed in this learning task, and we
discuss the operational interpretations of the $\ell^1$ and $\ell^2$ error
metrics. In contrast to previous works, our learning protocol requires neither
geometric locality nor any other relaxed locality conditions.
</summary>
    <author>
      <name>Muzhou Ma</name>
    </author>
    <author>
      <name>Steven T. Flammia</name>
    </author>
    <author>
      <name>John Preskill</name>
    </author>
    <author>
      <name>Yu Tong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30+12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18927v1</id>
    <updated>2024-10-24T17:14:40Z</updated>
    <published>2024-10-24T17:14:40Z</published>
    <title>SafeBench: A Safety Evaluation Framework for Multimodal Large Language
  Models</title>
    <summary>  Multimodal Large Language Models (MLLMs) are showing strong safety concerns
(e.g., generating harmful outputs for users), which motivates the development
of safety evaluation benchmarks. However, we observe that existing safety
benchmarks for MLLMs show limitations in query quality and evaluation
reliability limiting the detection of model safety implications as MLLMs
continue to evolve. In this paper, we propose \toolns, a comprehensive
framework designed for conducting safety evaluations of MLLMs. Our framework
consists of a comprehensive harmful query dataset and an automated evaluation
protocol that aims to address the above limitations, respectively. We first
design an automatic safety dataset generation pipeline, where we employ a set
of LLM judges to recognize and categorize the risk scenarios that are most
harmful and diverse for MLLMs; based on the taxonomy, we further ask these
judges to generate high-quality harmful queries accordingly resulting in 23
risk scenarios with 2,300 multi-modal harmful query pairs. During safety
evaluation, we draw inspiration from the jury system in judicial proceedings
and pioneer the jury deliberation evaluation protocol that adopts collaborative
LLMs to evaluate whether target models exhibit specific harmful behaviors,
providing a reliable and unbiased assessment of content security risks. In
addition, our benchmark can also be extended to the audio modality showing high
scalability and potential. Based on our framework, we conducted large-scale
experiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,
GPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs
and instantiated several insights on MLLM safety performance such as image
quality and parameter size.
</summary>
    <author>
      <name>Zonghao Ying</name>
    </author>
    <author>
      <name>Aishan Liu</name>
    </author>
    <author>
      <name>Siyuan Liang</name>
    </author>
    <author>
      <name>Lei Huang</name>
    </author>
    <author>
      <name>Jinyang Guo</name>
    </author>
    <author>
      <name>Wenbo Zhou</name>
    </author>
    <author>
      <name>Xianglong Liu</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18926v1</id>
    <updated>2024-10-24T17:13:39Z</updated>
    <published>2024-10-24T17:13:39Z</published>
    <title>LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor
  Search</title>
    <summary>  Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets.
</summary>
    <author>
      <name>Elias Jääsaari</name>
    </author>
    <author>
      <name>Ville Hyvönen</name>
    </author>
    <author>
      <name>Teemu Roos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NeurIPS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18924v1</id>
    <updated>2024-10-24T17:12:51Z</updated>
    <published>2024-10-24T17:12:51Z</published>
    <title>Swarm manipulation: An efficient and accurate technique for multi-object
  manipulation in virtual reality</title>
    <summary>  The theory of swarm control shows promise for controlling multiple objects,
however, scalability is hindered by cost constraints, such as hardware and
infrastructure. Virtual Reality (VR) can overcome these limitations, but
research on swarm interaction in VR is limited. This paper introduces a novel
Swarm Manipulation interaction technique and compares it with two baseline
techniques: Virtual Hand and Controller (ray-casting). We evaluated these
techniques in a user study ($N$ = 12) in three tasks (selection, rotation, and
resizing) across five conditions. Our results indicate that Swarm Manipulation
yielded superior performance, with significantly faster speeds in most
conditions across the three tasks. It notably reduced resizing size deviations
but introduced a trade-off between speed and accuracy in the rotation task.
Additionally, we conducted a follow-up user study ($N$ = 6) using Swarm
Manipulation in two complex VR scenarios and obtained insights through
semi-structured interviews, shedding light on optimized swarm control
mechanisms and perceptual changes induced by this interaction paradigm. These
results demonstrate the potential of the Swarm Manipulation technique to
enhance the usability and user experience in VR compared to conventional
manipulation techniques. In future studies, we aim to understand and improve
swarm interaction via internal swarm particle cooperation.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Jin-Du Wang</name>
    </author>
    <author>
      <name>John J. Dudley</name>
    </author>
    <author>
      <name>Per Ola Kristensson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, accepted at Computers &amp; Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18923v1</id>
    <updated>2024-10-24T17:11:52Z</updated>
    <published>2024-10-24T17:11:52Z</published>
    <title>SegLLM: Multi-round Reasoning Segmentation</title>
    <summary>  We present SegLLM, a novel multi-round interactive reasoning segmentation
model that enhances LLM-based segmentation by exploiting conversational memory
of both visual and textual outputs. By leveraging a mask-aware multimodal LLM,
SegLLM re-integrates previous segmentation results into its input stream,
enabling it to reason about complex user intentions and segment objects in
relation to previously identified entities, including positional,
interactional, and hierarchical relationships, across multiple interactions.
This capability allows SegLLM to respond to visual and text queries in a
chat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM
outperforms existing methods in multi-round interactive reasoning segmentation
by over 20%. Additionally, we observed that training on multi-round reasoning
segmentation data enhances performance on standard single-round referring
segmentation and localization tasks, resulting in a 5.5% increase in cIoU for
referring expression segmentation and a 4.5% improvement in Acc@0.5 for
referring expression localization.
</summary>
    <author>
      <name>XuDong Wang</name>
    </author>
    <author>
      <name>Shaolun Zhang</name>
    </author>
    <author>
      <name>Shufan Li</name>
    </author>
    <author>
      <name>Konstantinos Kallidromitis</name>
    </author>
    <author>
      <name>Kehan Li</name>
    </author>
    <author>
      <name>Yusuke Kato</name>
    </author>
    <author>
      <name>Kazuki Kozuka</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures, 11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18922v1</id>
    <updated>2024-10-24T17:11:37Z</updated>
    <published>2024-10-24T17:11:37Z</published>
    <title>How to Design a Quantum Streaming Algorithm Without Knowing Anything
  About Quantum Computing</title>
    <summary>  A series of work [GKK+08, Kal22, KPV24] has shown that asymptotic advantages
in space complexity are possible for quantum algorithms over their classical
counterparts in the streaming model. We give a simple quantum sketch that
encompasses all these results, allowing them to be derived from entirely
classical algorithms using our quantum sketch as a black box. The quantum
sketch and its proof of correctness are designed to be accessible to a reader
with no background in quantum computation, relying on only a small number of
self-contained quantum postulates.
</summary>
    <author>
      <name>John Kallaugher</name>
    </author>
    <author>
      <name>Ojas Parekh</name>
    </author>
    <author>
      <name>Nadezhda Voronova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SOSA 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18921v1</id>
    <updated>2024-10-24T17:10:39Z</updated>
    <published>2024-10-24T17:10:39Z</published>
    <title>From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical
  Integrity on Faulty Mathematical Problems</title>
    <summary>  Consider the math problem: "Lily received 3 cookies from her best friend
yesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.
How many cookies does Lily have now?" Many large language models (LLMs) in
previous research approach this problem by calculating the answer "1" using the
equation "3 - 5 + 3." However, from a human perspective, we recognize the
inherent flaw in this problem: Lily cannot eat 5 cookies if she initially only
had 3. This discrepancy prompts a key question: Are current LLMs merely Blind
Solver that apply mathematical operations without deeper reasoning, or can they
function as Logical Thinker capable of identifying logical inconsistencies?
  To explore this question, we propose a benchmark dataset, FaultyMath, which
includes faulty math problems of rich diversity: i) multiple mathematical
categories, e.g., algebra, geometry, number theory, etc., ii) varying levels of
difficulty, and iii) different origins of faultiness -- ranging from violations
of common sense and ambiguous statements to mathematical contradictions and
more. We evaluate a broad spectrum of LLMs, including open-source,
closed-source, and math-specialized models, using FaultyMath across three
dimensions: (i) How accurately can the models detect faulty math problems
without being explicitly prompted to do so? (ii) When provided with hints --
either correct or misleading -- about the validity of the problems, to what
extent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy
are the explanations generated by LLMs when they recognize a math problem as
flawed? Through extensive experimentation and detailed analysis, our results
demonstrate that existing LLMs largely function as Blind Solver and fall short
of the reasoning capabilities required to perform as Logical Thinker.
</summary>
    <author>
      <name>A M Muntasir Rahman</name>
    </author>
    <author>
      <name>Junyi Ye</name>
    </author>
    <author>
      <name>Wei Yao</name>
    </author>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Guiling Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18919v1</id>
    <updated>2024-10-24T17:09:37Z</updated>
    <published>2024-10-24T17:09:37Z</published>
    <title>Optimizing Edge Offloading Decisions for Object Detection</title>
    <summary>  Recent advances in machine learning and hardware have produced embedded
devices capable of performing real-time object detection with commendable
accuracy. We consider a scenario in which embedded devices rely on an onboard
object detector, but have the option to offload detection to a more powerful
edge server when local accuracy is deemed too low. Resource constraints,
however, limit the number of images that can be offloaded to the edge. Our goal
is to identify which images to offload to maximize overall detection accuracy
under those constraints. To that end, the paper introduces a reward metric
designed to quantify potential accuracy improvements from offloading individual
images, and proposes an efficient approach to make offloading decisions by
estimating this reward based only on local detection results. The approach is
computationally frugal enough to run on embedded devices, and empirical
findings indicate that it outperforms existing alternatives in improving
detection accuracy even when the fraction of offloaded images is small.
</summary>
    <author>
      <name>Jiaming Qiu</name>
    </author>
    <author>
      <name>Ruiqi Wang</name>
    </author>
    <author>
      <name>Brooks Hu</name>
    </author>
    <author>
      <name>Roch Guerin</name>
    </author>
    <author>
      <name>Chenyang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SEC 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18918v1</id>
    <updated>2024-10-24T17:09:10Z</updated>
    <published>2024-10-24T17:09:10Z</published>
    <title>MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete
  Data</title>
    <summary>  Causal discovery in real-world systems, such as biological networks, is often
complicated by feedback loops and incomplete data. Standard algorithms, which
assume acyclic structures or fully observed data, struggle with these
challenges. To address this gap, we propose MissNODAG, a differentiable
framework for learning both the underlying cyclic causal graph and the
missingness mechanism from partially observed data, including data missing not
at random. Our framework integrates an additive noise model with an
expectation-maximization procedure, alternating between imputing missing values
and optimizing the observed data likelihood, to uncover both the cyclic
structures and the missingness mechanism. We demonstrate the effectiveness of
MissNODAG through synthetic experiments and an application to real-world gene
perturbation data.
</summary>
    <author>
      <name>Muralikrishnna G. Sethuraman</name>
    </author>
    <author>
      <name>Razieh Nabi</name>
    </author>
    <author>
      <name>Faramarz Fekri</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18917v1</id>
    <updated>2024-10-24T17:08:20Z</updated>
    <published>2024-10-24T17:08:20Z</published>
    <title>Using Parametric PINNs for Predicting Internal and External Turbulent
  Flows</title>
    <summary>  Computational fluid dynamics (CFD) solvers employing two-equation eddy
viscosity models are the industry standard for simulating turbulent flows using
the Reynolds-averaged Navier-Stokes (RANS) formulation. While these methods are
computationally less expensive than direct numerical simulations, they can
still incur significant computational costs to achieve the desired accuracy. In
this context, physics-informed neural networks (PINNs) offer a promising
approach for developing parametric surrogate models that leverage both
existing, but limited CFD solutions and the governing differential equations to
predict simulation outcomes in a computationally efficient, differentiable, and
near real-time manner. In this work, we build upon the previously proposed
RANS-PINN framework, which only focused on predicting flow over a cylinder. To
investigate the efficacy of RANS-PINN as a viable approach to building
parametric surrogate models, we investigate its accuracy in predicting relevant
turbulent flow variables for both internal and external flows. To ensure
training convergence with a more complex loss function, we adopt a novel
sampling approach that exploits the domain geometry to ensure a proper balance
among the contributions from various regions within the solution domain. The
effectiveness of this framework is then demonstrated for two scenarios that
represent a broad class of internal and external flow problems.
</summary>
    <author>
      <name>Shinjan Ghosh</name>
    </author>
    <author>
      <name>Amit Chakraborty</name>
    </author>
    <author>
      <name>Georgia Olympia Brikis</name>
    </author>
    <author>
      <name>Biswadip Dey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at the Data-driven and Differentiable Simulations,
  Surrogates, and Solvers (D3S3) Workshop at NeurIPS'2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18915v1</id>
    <updated>2024-10-24T17:05:34Z</updated>
    <published>2024-10-24T17:05:34Z</published>
    <title>Testing Support Size More Efficiently Than Learning Histograms</title>
    <summary>  Consider two problems about an unknown probability distribution $p$:
  1. How many samples from $p$ are required to test if $p$ is supported on $n$
elements or not? Specifically, given samples from $p$, determine whether it is
supported on at most $n$ elements, or it is "$\epsilon$-far" (in total
variation distance) from being supported on $n$ elements.
  2. Given $m$ samples from $p$, what is the largest lower bound on its support
size that we can produce?
  The best known upper bound for problem (1) uses a general algorithm for
learning the histogram of the distribution $p$, which requires
$\Theta(\tfrac{n}{\epsilon^2 \log n})$ samples. We show that testing can be
done more efficiently than learning the histogram, using only
$O(\tfrac{n}{\epsilon \log n} \log(1/\epsilon))$ samples, nearly matching the
best known lower bound of $\Omega(\tfrac{n}{\epsilon \log n})$. This algorithm
also provides a better solution to problem (2), producing larger lower bounds
on support size than what follows from previous work. The proof relies on an
analysis of Chebyshev polynomial approximations outside the range where they
are designed to be good approximations, and the paper is intended as an
accessible self-contained exposition of the Chebyshev polynomial method.
</summary>
    <author>
      <name>Renato Ferreira Pinto Jr.</name>
    </author>
    <author>
      <name>Nathaniel Harms</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18912v1</id>
    <updated>2024-10-24T17:02:52Z</updated>
    <published>2024-10-24T17:02:52Z</published>
    <title>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</title>
    <summary>  Videos of robots interacting with objects encode rich information about the
objects' dynamics. However, existing video prediction approaches typically do
not explicitly account for the 3D information from videos, such as robot
actions and objects' 3D states, limiting their use in real-world robotic
applications. In this work, we introduce a framework to learn object dynamics
directly from multi-view RGB videos by explicitly considering the robot's
action trajectories and their effects on scene dynamics. We utilize the 3D
Gaussian representation of 3D Gaussian Splatting (3DGS) to train a
particle-based dynamics model using Graph Neural Networks. This model operates
on sparse control particles downsampled from the densely tracked 3D Gaussian
reconstructions. By learning the neural dynamics model on offline robot
interaction data, our method can predict object motions under varying initial
configurations and unseen robot actions. The 3D transformations of Gaussians
can be interpolated from the motions of control particles, enabling the
rendering of predicted future object states and achieving action-conditioned
video prediction. The dynamics model can also be applied to model-based
planning frameworks for object manipulation tasks. We conduct experiments on
various kinds of deformable materials, including ropes, clothes, and stuffed
animals, demonstrating our framework's ability to model complex shapes and
dynamics. Our project page is available at https://gs-dynamics.github.io.
</summary>
    <author>
      <name>Mingtong Zhang</name>
    </author>
    <author>
      <name>Kaifeng Zhang</name>
    </author>
    <author>
      <name>Yunzhu Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://gs-dynamics.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18907v1</id>
    <updated>2024-10-24T16:59:26Z</updated>
    <published>2024-10-24T16:59:26Z</published>
    <title>SkillMimicGen: Automated Demonstration Generation for Efficient Skill
  Learning and Deployment</title>
    <summary>  Imitation learning from human demonstrations is an effective paradigm for
robot manipulation, but acquiring large datasets is costly and
resource-intensive, especially for long-horizon tasks. To address this issue,
we propose SkillMimicGen (SkillGen), an automated system for generating
demonstration datasets from a few human demos. SkillGen segments human demos
into manipulation skills, adapts these skills to new contexts, and stitches
them together through free-space transit and transfer motion. We also propose a
Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and
termination components from SkillGen datasets, enabling skills to be sequenced
using motion planning at test-time. We demonstrate that SkillGen greatly
improves data generation and policy learning performance over a
state-of-the-art data generation framework, resulting in the capability to
produce data for large scene variations, including clutter, and agents that are
on average 24% more successful. We demonstrate the efficacy of SkillGen by
generating over 24K demonstrations across 18 task variants in simulation from
just 60 human demonstrations, and training proficient, often near-perfect, HSP
agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also
demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.
Videos, and more at https://skillgen.github.io.
</summary>
    <author>
      <name>Caelan Garrett</name>
    </author>
    <author>
      <name>Ajay Mandlekar</name>
    </author>
    <author>
      <name>Bowen Wen</name>
    </author>
    <author>
      <name>Dieter Fox</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2024 Conference on Robot Learning (CoRL)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.18907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18906v1</id>
    <updated>2024-10-24T16:57:20Z</updated>
    <published>2024-10-24T16:57:20Z</published>
    <title>PRISM: A Methodology for Auditing Biases in Large Language Models</title>
    <summary>  Auditing Large Language Models (LLMs) to discover their biases and
preferences is an emerging challenge in creating Responsible Artificial
Intelligence (AI). While various methods have been proposed to elicit the
preferences of such models, countermeasures have been taken by LLM trainers,
such that LLMs hide, obfuscate or point blank refuse to disclosure their
positions on certain subjects. This paper presents PRISM, a flexible,
inquiry-based methodology for auditing LLMs - that seeks to illicit such
positions indirectly through task-based inquiry prompting rather than direct
inquiry of said preferences. To demonstrate the utility of the methodology, we
applied PRISM on the Political Compass Test, where we assessed the political
leanings of twenty-one LLMs from seven providers. We show LLMs, by default,
espouse positions that are economically left and socially liberal (consistent
with prior work). We also show the space of positions that these models are
willing to espouse - where some models are more constrained and less compliant
than others - while others are more neutral and objective. In sum, PRISM can
more reliably probe and audit LLMs to understand their preferences, biases and
constraints.
</summary>
    <author>
      <name>Leif Azzopardi</name>
    </author>
    <author>
      <name>Yashar Moshfeghi</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18904v1</id>
    <updated>2024-10-24T16:48:32Z</updated>
    <published>2024-10-24T16:48:32Z</published>
    <title>Modulated Adaptive Fourier Neural Operators for Temporal Interpolation
  of Weather Forecasts</title>
    <summary>  Weather and climate data are often available at limited temporal resolution,
either due to storage limitations, or in the case of weather forecast models
based on deep learning, their inherently long time steps. The coarse temporal
resolution makes it difficult to capture rapidly evolving weather events. To
address this limitation, we introduce an interpolation model that reconstructs
the atmospheric state between two points in time for which the state is known.
The model makes use of a novel network layer that modifies the adaptive Fourier
neural operator (AFNO), which has been previously used in weather prediction
and other applications of machine learning to physics problems. The modulated
AFNO (ModAFNO) layer takes an embedding, here computed from the interpolation
target time, as an additional input and applies a learned shift-scale operation
inside the AFNO layers to adapt them to the target time. Thus, one model can be
used to produce all intermediate time steps. Trained to interpolate between two
time steps 6 h apart, the ModAFNO-based interpolation model produces 1 h
resolution intermediate time steps that are visually nearly indistinguishable
from the actual corresponding 1 h resolution data. The model reduces the RMSE
loss of reconstructing the intermediate steps by approximately 50% compared to
linear interpolation. We also demonstrate its ability to reproduce the
statistics of extreme weather events such as hurricanes and heat waves better
than 6 h resolution data. The ModAFNO layer is generic and is expected to be
applicable to other problems, including weather forecasting with tunable lead
time.
</summary>
    <author>
      <name>Jussi Leinonen</name>
    </author>
    <author>
      <name>Boris Bonev</name>
    </author>
    <author>
      <name>Thorsten Kurth</name>
    </author>
    <author>
      <name>Yair Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18902v1</id>
    <updated>2024-10-24T16:48:12Z</updated>
    <published>2024-10-24T16:48:12Z</published>
    <title>LLMs for Extremely Low-Resource Finno-Ugric Languages</title>
    <summary>  The advancement of large language models (LLMs) has predominantly focused on
high-resource languages, leaving low-resource languages, such as those in the
Finno-Ugric family, significantly underrepresented. This paper addresses this
gap by focusing on V\~oro, Livonian, and Komi. We cover almost the entire cycle
of LLM creation, from data collection to instruction tuning and evaluation. Our
contributions include developing multilingual base and instruction-tuned
models; creating evaluation benchmarks, including the smugri-MT-bench
multi-turn conversational benchmark; and conducting human evaluation. We intend
for this work to promote linguistic diversity, ensuring that lesser-resourced
languages can benefit from advancements in NLP.
</summary>
    <author>
      <name>Taido Purason</name>
    </author>
    <author>
      <name>Hele-Andra Kuulmets</name>
    </author>
    <author>
      <name>Mark Fishel</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18900v1</id>
    <updated>2024-10-24T16:40:36Z</updated>
    <published>2024-10-24T16:40:36Z</published>
    <title>Comparative Analysis of Indicators for Multiobjective Diversity
  Optimization</title>
    <summary>  Indicator-based (multiobjective) diversity optimization aims at finding a set
of near (Pareto-)optimal solutions that maximizes a diversity indicator, where
diversity is typically interpreted as the number of essentially different
solutions. Whereas, in the first diversity-oriented evolutionary multiobjective
optimization algorithm, the NOAH algorithm by Ulrich and Thiele, the Solow
Polasky Diversity (also related to Magnitude) served as a metric, other
diversity indicators might be considered, such as the parameter-free Max-Min
Diversity, and the Riesz s-Energy, which features uniformly distributed
solution sets. In this paper, focusing on multiobjective diversity
optimization, we discuss different diversity indicators from the perspective of
indicator-based evolutionary algorithms (IBEA) with multiple objectives. We
examine theoretical, computational, and practical properties of these
indicators, such as monotonicity in species, twinning, monotonicity in
distance, strict monotonicity in distance, uniformity of maximizing point sets,
computational effort for a set of size~n, single-point contributions, subset
selection, and submodularity. We present new theorems -- including a proof of
the NP-hardness of the Riesz s-Energy Subset Selection Problem -- and
consolidate existing results from the literature. In the second part, we apply
these indicators in the NOAH algorithm and analyze search dynamics through an
example. We examine how optimizing with one indicator affects the performance
of others and propose NOAH adaptations specific to the Max-Min indicator.
</summary>
    <author>
      <name>Ksenia Pereverdieva</name>
    </author>
    <author>
      <name>André Deutz</name>
    </author>
    <author>
      <name>Tessa Ezendam</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <author>
      <name>Hèrm Hofmeyer</name>
    </author>
    <author>
      <name>Michael T. M. Emmerich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18895v1</id>
    <updated>2024-10-24T16:35:23Z</updated>
    <published>2024-10-24T16:35:23Z</published>
    <title>ArterialNet: Reconstructing Arterial Blood Pressure Waveform with
  Wearable Pulsatile Signals, a Cohort-Aware Approach</title>
    <summary>  Continuous arterial blood pressure (ABP) monitoring is invasive but essential
for hemodynamic monitoring. Recent techniques have reconstructed ABP
non-invasively using pulsatile signals but produced inaccurate systolic and
diastolic blood pressure (SBP and DBP) values and were sensitive to individual
variability. ArterialNet integrates generalized pulsatile-to-ABP signal
translation and personalized feature extraction using hybrid loss functions and
regularization. We validated ArterialNet using the MIMIC-III dataset and
achieved a root mean square error (RMSE) of 5.41 mmHg, with at least a 58%
lower standard deviation. ArterialNet reconstructed ABP with an RMSE of 7.99
mmHg in remote health scenarios. ArterialNet achieved superior performance in
ABP reconstruction and SBP and DBP estimations, with significantly reduced
subject variance, demonstrating its potential in remote health settings. We
also ablated ArterialNet architecture to investigate the contributions of each
component and evaluated its translational impact and robustness by conducting a
series of ablations on data quality and availability.
</summary>
    <author>
      <name>Sicong Huang</name>
    </author>
    <author>
      <name>Roozbeh Jafari</name>
    </author>
    <author>
      <name>Bobak J. Mortazavi</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
